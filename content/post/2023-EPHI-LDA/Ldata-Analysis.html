<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title> Longitudinal Data Analysis </title>
    <meta charset="utf-8" />
    <meta name="author" content=" Anteneh Tessema and Yebelay Berehan " />
    <script src="Ldata-Analysis_files/header-attrs/header-attrs.js"></script>
    <link href="Ldata-Analysis_files/remark-css/default.css" rel="stylesheet" />
    <link href="Ldata-Analysis_files/remark-css/rladies-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, title-slide

.title[
# <p><span style="color:white"> Longitudinal Data Analysis </span></p>
]
.author[
### <span style="color:#38BDDE"> Anteneh Tessema and Yebelay Berehan </span>
]
.institute[
### <p><span style="font-size: 80%;"> Ethiopian Public Health Institute (EPHI)
National Data Management Center for Health (NDMC)</span></p>
]
.date[
### <p><span style = 'font-size: 50%;'> August 24-27, 2023</p>
]

---




<div>
<style type="text/css">.xaringan-extra-logo {
width: 95px;
height: 95px;
z-index: 0;
background-image: url(Slides/Images/tidyverse.png);
background-size: contain;
background-repeat: no-repeat;
position: absolute;
top:1em;right:1em;
}
</style>
<script>(function () {
  let tries = 0
  function addLogo () {
    if (typeof slideshow === 'undefined') {
      tries += 1
      if (tries < 10) {
        setTimeout(addLogo, 100)
      }
    } else {
      document.querySelectorAll('.remark-slide-content:not(.title-slide):not(.inverse):not(.hide_logo)')
        .forEach(function (slide) {
          const logo = document.createElement('div')
          logo.classList = 'xaringan-extra-logo'
          logo.href = null
          slide.appendChild(logo)
        })
    }
  }
  document.addEventListener('DOMContentLoaded', addLogo)
})()</script>
</div>





####	Topics to be covered 

** &lt;span style="color:purple"&gt;Day 1: Continuous Longitudinal Data&lt;/span&gt;**
  *  &lt;span style="color:red"&gt;Objectives&lt;/span&gt; 
  *  &lt;span style="color:red"&gt;Introduction to longitudinal data analysis&lt;/span&gt; 
  *  &lt;span style="color:red"&gt;Features of longitudinal data&lt;/span&gt; 
  * &lt;span style="color:red"&gt;Motivating examples&lt;/span&gt;
  * &lt;span style="color:red"&gt;Cross sectional vs longitudinal data&lt;/span&gt;
  * &lt;span style="color:red"&gt;Simple methods&lt;/span&gt;
  * &lt;span style="color:red"&gt;Exploratory data analysis&lt;/span&gt;
  
** &lt;span style="color:purple"&gt;Day 2: Modelling of Longitudinal Data&lt;/span&gt;**
  *	Linear Mixed Effects Models
  * Marginal Models
  * Estimation of Marginal model
  
  
---
** &lt;span style="color:purple"&gt;Day 3: Methods for Discrete Data&lt;/span&gt; **
  * &lt;span style="color:blue"&gt;Generalized Estimating Equations (GEE)&lt;/span&gt;
  * &lt;span style="color:blue"&gt;Generalized Linear Mixed Models (GLMM)&lt;/span&gt;
 
** &lt;span style="color:purple"&gt;Day 4: Addressing Missing Data in Longitudinal Studies&lt;/span&gt;**
  * Types of Missing Data Mechanisms (MCR, MR, NMR)
  * Handling Missing Data: Multiple Imputation
  * Handling Missing Data: Weighted GEE

---

#### Goal of the training

The goal of this training is to:

- Provide an overview of fundamental statistical models and methods for the analysis of **longitudinal data**, including key theoretical results presented.

- Focus on the practical implementation of these methods in **R **.

- Help **trainees** gain a comprehensive understanding of the properties and use of modern methods for **longitudinal data analysis**.

- Enable trainees to pose scientific questions within the context of appropriate statistical models and **carry out and interpret analyses effectively**.

---

#### Primary Objectives ... (1)


- Understand how longitudinal data differs from cross sectional data.

- Explain the consequences of ignoring correlated observations.

- Appreciate the merits of longitudinal data analysis.

- Apply graphical techniques to explore repeated/dependent/clustered data.

- Discuss different model families.

- Analysis and interpret results from longitudinal studies.


---

#### Primary Objectives ... (2) 

- Understand LDA Models for Gaussian and non Gaussian data.

- Be familiar with theoretical background of statistical techniques used for analyzing longitudinal and handling of missing data.

- Translate statistical theory into practical application.

- Prepare scientific reports describing methods used for analysis, results obtained and their interpretation.

- Communicate methods used and the clinical/scientific meaning of the results from a longitudinal data analysis and defending their analysis.



---

#### Primary Objectives ... recap 
- Understand the effect of non-independence in longitudinal data.

- Recognize limitations of classical analysis methods in longitudinal studies.

- Explore and analyse the marginal distribution of longitudinal data.

- Learn and apply methods for analyzing continuous outcomes using linear mixed effects models.

- Utilize methods for analyzing discrete data in longitudinal studies using GEE and GGLMM.

- Gain knowledge about missing data mechanisms and techniques for handling them.

- Statistical computing packages: R, Stata and SAS



---
class: inverse,  middle

# Day 1

## Continuous Longitudinal Data
 
 * Introduction to longitudinal data analysis
 * Motivational Examples
 
---


#### Introduction to longitudinal data analysis
  
##### Repeated measures ... (1)

- Statistical techniques like ANOVA and regression have a basic assumption that the residual or error terms are independent and identically distribution (iid).

- In applied sciences, often confronted with the collection of correlated data.

 - The term embraces a multitude of data structures such as multivariate observations, clustered data, repeated measurements,longitudinal data &amp; spatially correlated data.

- The distinguishing feature of repeated data is that they are correlated.

---

##### Repeated measures ... (2)

- Familiar examples of clustered data are families, schools, hospitals, towns, litters, ...
 - In each of these examples, a cluster is a collection of sub units on which observations are made.

- Another form of clustering arises when data are measured repeatedly on the same unit.

 - When these repeated measurements are taken over time, it is called a longitudinal study (panel data).

 - When the correlation occurs over space, it is called a Spatial study.

---

#### Longitudinal data

- Special forms of repeated measurements.

- Longitudinal Studies: Studies in which individuals are measured repeatedly through time.

- Longitudinal data (LD) sets differ from time Series (TS) data sets.

 - LD: usually consists of a large number of a short series of time points.

 - TS: usually consists of a single, long series of time points.

- Examples of LD:

  - Monthly CD4 count (viral load) of a patient over time.

  - Psychological change of a patient.

  - The effect of a treatment to cure a disease over time.

---

#### Features of longitudinal data

- Defining feature of longitudinal studies is that measurements of the same individuals are taken repeatedly through time.

- Longitudinal studies allow direct study of change over time.

- Objective: to characterize the change in response over time and factors that influence change.

- With repeated measures on individuals, we can capture within-individual change.

- In longitudinal studies, the outcome variable can be:
  - Continuous (e.g., blood lead levels).
  - Binary (e.g., presence/absence).
  - Count (e.g., number of epileptic seizures).

- The data set can be incomplete (missing data/dropout).

- Subjects may be measured at different occasions.

- In this module we will master a set of statistical tools that can handle all of these cases.

- Emphasis on concepts, model building, software and interpretation.

---

#### Famous LD examples ... (1)

The Baltimore Longitudinal Study of Aging (BLSA)

- BLSA: Ongoing, multidisciplinary observational study, started in 1958

- Objective : characterize the many aspects of the aging process and learn how people can adapt to aging

- Volunteers return approximately every 2 years for 3 days of biomedical and psychological examinations

- at first only males (over 1500 by now), later also females

- an average of about 20 years of follow up

 - NCT00233272

---

#### Famous LD examples ... (2)

Indonesian children’s health study (ICHS)

- Interest is to investigate the association between risk of respiratory illness and vitamin A deficiency

- 250 children followed

- Multivariate data have received most attention in the stat. literature.

- Remarkable developments in statistical methodology for LDA have been done in the past 30 years.

---

#### &lt;span style="color:purple"&gt; Advantages of modern longitudinal methods ... (1)&lt;/span&gt; 

- You have much more flexibility in research design.

   - Not everyone needs the same rigid data collection schedule.
   - Not everyone needs the same number of measurements—can use all cases, even those with just one measurement!


- You can identify temporal patterns in the data.

   - Does the outcome increase, decrease, or remain stable over time?
   - Is the general pattern linear or non-linear?
   - Are there abrupt shifts at substantively interesting moments?

- You can include time-varying predictors.

- Can provide information about individual change.

---

#### &lt;span style="color:purple"&gt; Advantages of modern longitudinal methods ... (2)&lt;/span&gt; 

- You can include interactions with time (to test whether a predictor’s effect varies over time).
   - Some effects dissipate—they wear off.
   - Some effects increase—they become more important.
   - Some effects are especially pronounced at particular times.

- Can provide more efficient estimators than cross-sectional designs with the same number and pattern of observations.

- Can separate aging effects (changes over time within individuals) from cohort effects (differences between subjects at baseline) ⇒ cross-sectional design can’t do this.

---

#### &lt;span style="color:purple"&gt;Challenges of Longitudinal Data Analysis&lt;/span&gt;

- Observations are not, by definition, independent → must account for dependency in data.

- Analysis methods not as well developed, especially for more sophisticated models.

- Difficulty of using state-of-the-art software.

- Computationally intensive.

- Unbalanced designs, missing data, attrition.

- Carry-over effects (when the repeated factor is condition or treatment, not time).

---

#### Recap:

- Longitudinal studies:
  - Measurements of the same individuals are taken repeatedly through time.
  - Allow direct study of change over time.
  - We can capture within-individual change.
- Objective: to characterize the change in response over time and factors that influence change.

---

#### Motivating examples

**&lt;span style="color:blue"&gt;The Jimma Infant Survival Data&lt;/span&gt;**

- A follow-up study of newborn infants in Southwest Ethiopia.
- Wide ranges of data were collected on the following characteristics:
  - Basic demographic information.
  - Feeding practice.
  - Anthropometric measurements.

- Infants were followed for 12 months.
- Measurements were taken at seven time points from each child.


```r
library(readxl)
Infant &lt;- read_xls("Data/Infant.xls")
Infant$sex &lt;- factor(Infant$sex, levels= c(1, 0), labels= c("male", "female"))
```

---


```r
library(dplyr)
output &lt;- table(Infant$age) %&gt;%
  as.data.frame() %&gt;%
  mutate(Time = as.numeric(as.character(Var1)),
         N = Freq,
         Percentage =round((Freq/Freq[1])*100,1)) %&gt;%
  select(Time, N, Percentage)
print(output)
```

.pull-left[

```
##   Time   N Percentage
## 1    0 971      100.0
## 2    2 949       97.7
## 3    4 894       92.1
## 4    6 857       88.3
## 5    8 833       85.8
## 6   10 811       83.5
## 7   12 784       80.7
```
]

.pull-right[
- Infants were followed during 12 months.
- Measurements were taken at seven time points from each child, resulting in a maximum of seven measurements per subject.
- For our purpose, we will consider the variable weight. 
- Due to a variety of reasons, 80.7% continues up to the end of the study.
]

---

**&lt;span style="color:blue"&gt;The Income Dynamics (PSID) Study&lt;/span&gt;**

- The Panel Study of Income Dynamics (PSID) began in 1968 and is still continuing.
- It is the longest-running longitudinal household survey in the world.
- The PSID is a longitudinal study of a representative sample of U.S. individuals.
- The data that represents a small subset of this data (1661 observations) is available in R software under the library "faraway".
- Variables included in the dataset: Age, Education (years of education), Sex, and Annual Income.

**The Question of Interest**

The PSID dataset raises the following questions:

- Is there a change in income over the years?
- Is there variation in income by sex?

---

**PSID: Profile Plots for 20 Subjects**


```r
library(faraway);library(lattice)
data(psid)
mypsid&lt;-subset (psid, (subset=(person &lt;4)))
mypsid1&lt;-subset(mypsid, (subset=(year &lt; 75)))
xyplot(income ~ year , psid, type="l", subset=( person &lt; 20),strip=TRUE)
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

- In the PSID dataset, some individuals have a slowly increasing income, while others have more erratic incomes.
- There is small variation at the beginning compared to the end.
- Income may possibly vary by sex, so we may need profile plots by sex.

---

**PSID: Profile of Income by Sex**


```r
xyplot(income ~ year | sex, psid, type="l", subset=( person &lt; 500),strip=TRUE)
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

- The variation for males is higher than that of females.
- Income data for males is more erratic.
- Variation at the beginning is smaller than the variation at the end for both groups.

---

**&lt;span style="color:blue"&gt;Orthodontic Growth Data**

- Taken from Potthoff and Roy, Biometrika (1964).

- The distance from the center of the pituitary to the maxillary fissure was recorded at ages 8, 10, 12, and 14, for 11 girls and 16 boys.

- Data were collected by orthodontists from x-rays of the children’s skulls.

- 108 total records were grouped into 27 groups by Subject.

- This is an example of balanced repeated measures data, with a single level of grouping (Subject).

- Research question: Is dental growth related to gender?

---

**The Orthodontic Growth Data: Individual Profiles by Sex**


```r
library(readr); library(dplyr)
growth &lt;- read_csv("Data/growth2.csv") 
  growth$age &lt;- as.factor(growth$age); 
  growth$Sex &lt;- factor(growth$sex, levels= c(1, 2), labels= c("male", "female"))
 growth %&gt;%  xyplot(measure ~ age|Sex, data = ., groups = ind, type = "l",
         xlab ="age",ylab= "distance")
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

- Much variability between children and considerable variability within children.
- Fixed number of measurements per subject and measurements taken at fixed time points.

---

**The Orthodontic Growth Data: Mean Distance Profile by Sex**


```r
mean1&lt;-tapply(growth$measure, growth$age, mean)
age1&lt;-as.numeric(unique(growth$age)) %&gt;% sort()
plot(age1, mean1, type="l",ylim=c(20,30), xlab="age", ylab=" The mean distance", 
     lwd=3, main=" The mean profile of the growth data set")
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

- The relationship between age and distance appears to be linear.

- It appears that there is a linear growth pattern.


---

#### Cross-sectional versus longitudinal data

Cross-sectional data and longitudinal data are two primary types of data used in statistical analysis:

- Cross-sectional data: Collected at a specific point in time and involves observations of different individuals at that particular time.

- Longitudinal data: Collected over an extended period, involving repeated observations of the same individuals over time.

---

#### t-test example

**Diastolic Blood Pressures (DBP) from the Captopril Data**

- Consider the DBP.

- It includes 15 patients with hypertension.

- The response of interest was the supine blood pressure before and after treatment with CAPTOPRIL.

- Research question: &lt;span style="color:red"&gt;How does treatment affect BP?&lt;/span&gt;

---


```r
library(tidyverse)
before &lt;- c(130, 122, 124, 104, 112, 101, 121, 124, 115, 102, 98, 119, 106, 107, 100)
after &lt;- c(125, 121, 121, 106, 101, 85, 98, 105, 103, 98, 90, 98, 110, 103, 82)
data &lt;- data.frame( group = rep(c("Before", "After"), each = 15),
  dbp = c(before, after))
paired &lt;- data %&gt;% group_by(group) %&gt;%summarize(Mean = mean(dbp),  N = n(),
      Std_Deviation = sd(dbp), Std_Error_Mean = sd(dbp)/sqrt(n()))
print(paired)
```

```
## # A tibble: 2 × 5
##   group   Mean     N Std_Deviation Std_Error_Mean
##   &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt;          &lt;dbl&gt;
## 1 After   103.    15          12.6           3.24
## 2 Before  112.    15          10.5           2.70
```

- Paired data analysis: Examines related variables within the same subjects.
- DBP: Analysed before and after treatment.
- Average decrease: More than 9 mmHg after treatment.
- Classical analysis: Compares measurements within each subject/participant.
`\(d_i = Y_{i1}-Y_{i2}, i = 1, 2, ..., 15\)`
- Focus: Changes from before to after treatment.
- Testing for treatment effect: Assesses if the average difference equals zero.
- Paired observations: The simplest case of longitudinal data
- Much variability between subjects

---


#### Unpaired, two sample t-test

- What if we had ignored the paired nature of the data?
- We then could have used a two sample (unpaired) t test to compare the average BP of untreated patients (controls) with treated patients.
- We would still have found a significant difference (p= 0.0377), but the p value would have been more than 30 30×larger compared to the one obtained using the paired t test (p=0.001).


Conclusion:
- The two sample t test does not take into account the fact that the 30 measurements are not independent observations.
- Illustration: classical statistical models which assume independent observations will not be valid for the analysis of longitudinal data

---

#### Cross-sectional vs longitudinal data

- Cross-sectional data refers to the data collected at a specific point in time.
- Observations from cross-sectional data are uncorrelated.
- Longitudinal data refers to measurements made repeatedly over time to study how the subjects/patients/participants evolve over time.
 - That means the concern of longitudinal data analysis is change over time.
- In longitudinal study, the measurements made for participants over a period of time are correlated.
- Suppose it is of interest to study the relation between some response Y and age.
- A cross-sectional study yields the following data:

&lt;img src="Image/Y_age.png" width="38%" style="display: block; margin: auto;" /&gt;
- Graph suggesting a negative relation between Y and age

---

#### Cross-sectional vs longitudinal data

- Exactly the same observations could also have been obtained in a longitudinal study, with 2 measurements per subject.
- First case:
&lt;img src="Image/cross_sec.png" width="55%" style="display: block; margin: auto;" /&gt;
- Graph suggesting a negative cross-sectional relation but a positive longitudinal trend

---

#### Cross-sectional vs longitudinal data

- Second case:
&lt;img src="Image/logitu.png" width="55%" style="display: block; margin: auto;" /&gt;
- Graph suggesting the cross-sectional as well as longitudinal trend to be negative.
  
- **Conclusion:** Longitudinal data allow distinguishing differences between subjects from changes within subjects.

---

#### Longitudinal data: wide/broad form

.pull-left[
- Wide format of data

| Subject | Time 1 (y) | Time 2 (y) | Time 3 (y) | Time 1 (x) | Time 2 (x) | Time 3 (x) |
| ------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |
| 1       | 10         | 6          | 6          | 4          | 4          | 4          |
| 2       | 7          | 5          | 3          | 2          | 2          | 2          |
| 3       | 12         | 9          | 8          | 6          | 6          | 6          |
| 4       | 11         | 14         | 16         | 8          | 8          | 8          | 


- 

```r
# Using pivot_wider 
wide_data &lt;- long_data %&gt;%
  pivot_wider(names_from = Time, 
              values_from = c(y, x))
```
]

.pull-right[

- Long format of data

subject | time | y | x
------- | ---- | - | -
1 | 1 | 10 | 4
1 | 2 | 6 | 4
1 | 3 | 6 | 4
2 | 1 | 7 | 2
2 | 2 | 5 | 2
2 | 3 | 3 | 2
3 | 1 | 12 | 6
3 | 2 | 9 | 6
3 | 3 | 8 | 6
4 | 1 | 11 | 8
4 | 2 | 14 | 8
4 | 3 | 16 | 8
]



---

#### Longitudinal data

- With LD: multiple measurements taken on each subject.
- You not only can examine the differences between subjects, but you can also examine the change within subjects across time.
- The number of observations is not the number of subjects but rather the number of measurements taken on all the subjects.
- There are three repeated measurements on each subject, you now have 12 observations.
- How does this change your variance-covariance matrix?

---

#### Variance-covariance matrix for longitudinal data

&lt;img src="Image/vcov.png" width="55%" style="display: block; margin: auto;" /&gt;

- 3 repeated measurements on each subject:
  - We now have 12 observations and a 12x12 variance-covariance matrix.
- For a simple longitudinal model, the matrix is a block-diagonal matrix.
- The matrix is a block-diagonal matrix:
  - Observations within each block are assumed to be correlated.
  - Observations outside of the blocks are assumed to be independent (subjects are still assumed to be independent of each other).

---

#### Introduction to longitudinal data analysis

**&lt;span style="color:purple"&gt;Longitudinal data:&lt;/span&gt;** data in the form of repeated measurements over time or other factors on each individual or unit in a sample from a population of interest.

Examples:
- Weekly measurements of growth on experimental plots with different fertilizers.
- Monthly measurements of viral load on HIV-infected patients with different treatment regimens.

**Defining Characteristic**

The same response or outcome is measured repeatedly on each unit.

**Scientific Questions**

- How mean response differs across treatments or other factors.
- How the change in mean response over time differs.
- Other features of the relationship between response/outcome and time.

---

#### Required statistical model

A statistical model that acknowledges this data structure in which the questions can be formalized and associated specialized methods of analysis based on the model.

- Longitudinal data/studies have become increasingly common and widespread across various scientific disciplines.

**Terminology**

- Longitudinal data refers to data in the form of repeated measurements that might be over time but could also be over some other set of conditions.
- Time is most often the condition of measurement.
- "Response" and "outcome" are used interchangeably to denote the repeated measurement or outcome of interest.
- "Participant", "Unit", "individual" and "subject" are used interchangeably to refer to the entity being measured.

**Applications**

We consider several applications that exemplify longitudinal data situations and the range of ways data are collected and types of responses and questions of interest.

---

#### Simple methods

-	Introduction 
 - Overview of frequently used methods
-	Summary statistics
- Practical using R


---

#### Simple methods: introduction

- The reason why classical statistical techniques fail in the
context of longitudinal data is that observations within participants
are correlated.

  - often the correlation between two repeated measurements decreases as
the time span between those measurements increases

- The paired t-test accounts for this by considering participant
specific differences `\(∆_i = Y_{i1} − Y_{i2}\)`

 - This reduces the number of measurements to just one per participant, which
implies that classical techniques can be applied again.

---

#### Overview of frequently used methods

- In the case of more than 2 measurements per participant, similar simple techniques are often applied to reduce the number of measurements for the `\(i^{th}\)` participant, from `\(n_i\)` to `\(1\)`.

**Some Examples:**

- Analysis at each time point separately

- Analysis of Area Under the Curve (AUC)

- Analysis of endpoints

- Analysis of increments

- Analysis of covariance

---

#### Sesame data

- Cross-year by locations trial of sesame genotypes

- Sesame is a short day plant &amp; sensitive to photo-period, temperature, and moisture stress.

- The yield is reported to vary across years and locations.

- Variation in rainfall could lead to the change of yield across locations.

- The study area was characterized by uni-modal rainfall pattern 
   - low in amount, short in duration, and poor in distribution within a short distance.

- To examine the effect of Genotype X Environment Interaction, 13 genotypes were tested across three locations over three years.

- Aim: to see the change in yield of different genotypes over years.

---

#### The sesame Data

&lt;img src="Image/sesame_prof.png" width="45%" style="display: block; margin: auto;" /&gt;
- Three years average yield over the three locations by thirteen genotypes
- Sesame Data: Profile plot by Year and Location

---

#### Sesame Data: Profile plot by Year and Location

.pull-left[
&lt;img src="Image/sesame_area.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.pull-right[

- Fixed # of measurements per genotypes

- It seems that the yield on the first year is &gt;&gt; yield on the `\(2^{nd}\)` year for the majority of the observations.

- Some lines on the plot show an increasing trend

- Some genotypes have a larger yield than others

- Variability between genotypes

- Variability within genotypes

- May be it is good to see the mean profile plot by location
]

---

#### Mean profile of sesame yield by location

&lt;img src="Image/sesame_mean.png" width="45%" style="display: block; margin: auto;" /&gt;
- Area2: the mean yield is almost constant over time

- Area1 &amp; Area3: the mean yield decrease from Yr 1 to Yr 2 and then increase to Yr 3.

---

#### Overview of frequently used methods

#### 1. Analysis at each time point separately

- The data are analysed at each occasion separately.

- Example: Use the sesame data set to analyze the number of days to maturity using one-sample t-test for each location at each time point.


&lt;img src="Image/fig14.png" width="90%" style="display: block; margin: auto;" /&gt;

---

**Summary of Days to Maturity**

- A simple summary of days to maturity for each location at a given time point.

- Comparison of different locations for year 1.

- Performed two-sample t-test, despite having three locations.


&lt;img src="Image/fig15.png" width="80%" style="display: block; margin: auto;" /&gt;
- Problem of multiple testing since multiple t-tests are used!

---

#### Why is multiple testing problem?

- Multiple testing: Conducting multiple t-tests leads to inflation of Type I error.

- Experiment-wise Type I error rate: Probability of falsely rejecting at least one null hypothesis among multiple tests.

- `\(\alpha = 5\% \Longrightarrow\)` one trial in 20 will falsely claim that a difference exist when there is none

- Let’s consider a case where you have 20 hypotheses to test at a significance level of 0.05.

- Prob . at least one sig. result (experiment wise Type I error rate) `\(= 1−(1−\alpha)^{20} = 0.64\)` 
- `\(\Longrightarrow\)` with 20 tests being considered, we have a 64% chance of observing at least one significant result, even if all of the tests are actually not significant

---

#### Experiment wise Type I Error


| # of Comparisons (K) | Experiment-wise Type I Error |
|---------------------|-----------------------------|
|         1           |             0.05            |
|         2           |            0.0975           |
|         3           |            0.1426           |
|         5           |            0.2262           |

- With 5 tests at 5% significance level, there's a 22.6% chance of observing at least one significant result even if all tests are not significant.

- If 3 comparisons are performed at 5%, the overall significance will be increased more that 14%
- Increasing the number of comparisons also increases the overall significance.

---

#### Experiment-wise Type I Error

**Advantages of Analysis at Each Time Point**

- Simple to interpret.
- Uses all available data.

**Disadvantages of Analysis at Each Time Point**

- Does not consider overall differences.
- Does not allow studying evolutionary differences.
- Problem of multiple testing.
- Possible issues with missing data.


---

#### Analysis of Area Under the Curve (AUC)

- For each participant, the area under its curve is calculated:
`$$AUC_i=(t_{i2}-t_{i1})*(y_{i2}-y_{i1})/2+(t_{i3}-t_{i2})*(y_{i3}-y_{i2})/2+ ...$$`

- Afterwards, these `\(AUC_i\)` are analyzed

- Ex: we use the days to CFU data to calculate the area under the curve.

##### Advantages

- no problems of multiple testing
- does not explicitly assume balanced data
- compares ‘overall’ differences

##### Disadvantage

- uses only partial information: `\(AUC_i\)`
- participants could have the same AUC but completely different profiles
- possible problems with missing data


---

#### Analysis of endpoints 

- General Idea : Assess differences only on the last time point

- In randomised studies, there are no systematic differences at baseline.

- Hence, ‘treatment’ effects can be assessed by only comparing the measurements at the last occasion

##### Advantages
- no problems of multiple testing
- does not explicitly assume balanced data

##### Disadvantages
- uses only partial information
- only valid for large data sets
- the last time point must be the same for all participants
- does not consider ‘overall’ differences
- possible problems with missing data

---

#### Analysis of increments

- A simple method to compare evolutions between participants,
correcting for differences at baseline, is to analyze the
participant-specific changes: `\(y_{in_i}-y_{i1}\)`


##### Advantages
- no problems of multiple testing
- does not explicitly assume balanced data

##### Disadvantage
- uses only partial information
- the last time point must be the same for all participants
- possible problems with missing data

---

#### Analysis of covariance

- Another way to analyse endpoints, correcting for differences at baseline, is to use analysis of covariance techniques, where the first measurement is included as covariate in the model.

##### Advantages:

- no problems of multiple testing
- does not explicitly assume balanced data

##### Disadvantages:
- uses only partial information: `\(y_{i1}\)` and `\(y_{in_i}\)`
- does not take into account the variability of `\(y_{i1}\)` 

---

#### Summary Statistics

- The AUC, endpoints and increments are examples of summary statistics. 
   
   - Such summary statistics summarise the vector of repeated
measurements for each participant separately.

- This leads to the following general procedure:
   - Step 1: Summarize data of each participant into one statistic, a
summary statistic
   - Step 2: Analyze the summary statistics, e.g. analysis of covariance
to compare groups after correction for important covariates

- This way, the analysis of longitudinal data is reduced to the analysis of independent observations, for which classical statistical procedures are available.

- These techniques are based on extensions of simple regression
models for univariate data

---

- However, all these methods have the disadvantage that (lots of) information is lost
- Further, they often do not allow to draw conclusions about the way the endpoint has been reached

&lt;img src="Image/fig8.png" width="50%" style="display: block; margin: auto;" /&gt;

- This has led to the development of statistical techniques that overcome
these disadvantages


---

### &lt;span style="color:red"&gt;Exploratory data analysis&lt;/span&gt;

##### Introduction

Exploratory analysis comprises techniques to visualize patterns in the data.

Data analysis must begin by making displays that expose patterns relevant to the scientific question.

A linear mixed model makes assumptions about:
- &lt;span style="color:blue;"&gt;mean structure&lt;/span&gt;: (non-)linear, covariates, ...
- &lt;span style="color:blue;"&gt;variance function&lt;/span&gt;: constant, quadratic, ...
- &lt;span style="color:blue;"&gt;correlation structure&lt;/span&gt;: constant, serial, ...
- &lt;span style="color:blue;"&gt;subject-specific profiles&lt;/span&gt;: linear, quadratic, ...

In practice, linear mixed models are often obtained from a two-stage model formulation.

However, this may or may not imply a valid marginal model.

---

#### Exploratory data analysis.... (2)

Longitudinal data analysis, like other statistical methods, has two components which operate side by side:
- exploratory and
- confirmatory analysis.

Exploratory analysis comprises techniques to visualize patterns in the data.
Confirmatory analysis is judicial work, weighing evidence in data for, or against hypotheses.

Data analysis must begin by making displays that expose patterns relevant to the scientific question.

The best methods are capable of uncovering patterns which are unexpected.

In this regard graphical displays are so important. At this stage, the following guidelines are very useful.

---

#### Jimma infant data ... (1)

Follow-up study of newborn infants in Southwest Ethiopia.

Wide ranges of data were collected on the following characteristics:
- basic demographic information,
- feeding practice,
- anthropometric measurements, ...

Infants were followed during 12 months.
Measurements were taken at seven time points every two months from each child.
Weight was one of the variables recorded at each visit.

Research question: &lt;span style="color:red;"&gt;How does weight change over time?&lt;/span&gt;
---

#### Jimma infant data ... (2)

The individual profiles support a &lt;span style="color:purple;"&gt;random-intercepts model&lt;/span&gt;


```r
attach(Infant); library(lattice)
mydata1&lt;-as.data.frame(Infant)
xyplot(weight ~ factor(age), group = ind, lty=1 ,  
       data =mydata1[sample(nrow(mydata1), 800),], main="a. Individual profile plot",
       xlab = "Time", ylab = "Weight in kg", type = "a", lines = TRUE)
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;


---

#### Conclusions from the profile

- Much &lt;span style="color:blue;"&gt;variability between children&lt;/span&gt;

- Considerable &lt;span style="color:blue;"&gt;variability within subjects&lt;/span&gt;

- Fixed number of measurements per subject
- Measurements taken at fixed time points

---

#### Mean profile

The mean profile can be plotted using the following R code:


```r
mean1&lt;-tapply(Infant$weight, Infant$age, mean, na.rm=T)
age1&lt;-as.numeric(unique(Infant$age))
plot(age1, mean1, type = "l", xlab = "Age", ylab = "Mean Weight", lwd = 2, 
     main = "The Mean Profile")
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;" /&gt;

---

#### Mean profile by sex

- The mean profiles by sex:

```r
interaction.plot(Infant$age, sex, Infant$weight, fun = mean, 
                 col = c("red", "blue"), xlab = "Age", ylab = "Weight", las = 1)
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-22-1.png" style="display: block; margin: auto;" /&gt;

---

#### Exploring the &lt;span style="color:blue"&gt;random effects&lt;/span&gt;

- The mean structure for linear mixed effect model can be determined based on the &lt;span style="color:red"&gt;random effects&lt;/span&gt;.

- Choosing which parameters in the model should have a &lt;span style="color:red"&gt;random-effect&lt;/span&gt; component included to account for between-group variation.

- The &lt;span style="color:purple"&gt;lmList&lt;/span&gt; function and the methods associated with it are useful for this.

- Continuing with the analysis of the &lt;span style="color:blue"&gt;Jimma infants&lt;/span&gt; data, we see from the individual profiles of these data that a simple linear regression model of &lt;span style="color:red"&gt;weight&lt;/span&gt; as a function of &lt;span style="color:red"&gt;age&lt;/span&gt; may be suitable.

---

#### Jimma infant survival

-The data was fitted this for each subject as follows:

```r
library(nlme)
fit &lt;- lmList(weight ~ age | ind, Infant)
fit
```

```
## Call:
##   Model: weight ~ age | ind 
##    Data: Infant 
## 
## Coefficients:
##      (Intercept)           age
## 1       4200.000  2.714286e+02
## 3       5435.714  1.821429e+02
## 4       4435.714  2.392857e+02
## 5       4139.286  3.196429e+02
## 6       4485.714  4.571429e+02
## 7       4400.000  3.428571e+02
## 8       4550.000  3.250000e+02
## 9       3792.857  3.250000e+02
## 10      4635.714  3.821429e+02
## 11      3417.143  5.592857e+02
## 12      3860.714  5.089286e+02
## 14      3407.143  2.607143e+02
## 15      4235.714  3.892857e+02
## 17      5346.429  1.946429e+02
## 18      4264.286  2.321429e+02
## 19      4392.857  4.107143e+02
## 20      4371.429  3.142857e+02
## 21      3900.000  2.785714e+02
## 22      2894.643  4.687500e+02
## 23      2780.000  8.400000e+02
## 24      4967.857  4.410714e+02
## 25      3639.286  3.625000e+02
## 26      4539.286  3.553571e+02
## 27      3907.143  4.678571e+02
## 29      3792.857  5.535714e+02
## 30      3982.143  4.125000e+02
## 31      3316.667  2.500000e+01
## 32      4042.857  4.285714e+02
## 33      4078.571  4.392857e+02
## 34      4150.000  8.000000e+02
## 35      3532.143  4.589286e+02
## 36      4375.000  2.803571e+02
## 37      2560.000  6.050000e+02
## 38      4746.429  5.160714e+02
## 39      4044.643  2.723214e+02
## 40      4932.143  4.303571e+02
## 42      3284.906  3.882075e+02
## 43      3991.925  4.102484e+02
## 44      3546.429  3.803571e+02
## 45      4267.857  4.839286e+02
## 46      4485.714  3.357143e+02
## 47      4660.714  3.732143e+02
## 48      2867.857  4.625000e+02
## 49      2500.000  2.500000e+02
## 50      3853.571  3.267857e+02
## 51      6309.938  3.931677e+02
## 53      4603.571  3.946429e+02
## 54      4023.602  3.046584e+02
## 55      2950.000  8.250000e+02
## 56      2903.571  4.017857e+02
## 57      4975.000  3.875000e+02
## 58      4560.714  4.303571e+02
## 59      3953.571  2.910714e+02
## 60      3900.000  3.357143e+02
## 61      4328.571  4.071429e+02
## 62      3796.429  3.625000e+02
## 63      3935.714  3.178571e+02
## 64      3442.857  4.071429e+02
## 65      4925.000  4.696429e+02
## 66      3753.571  3.625000e+02
## 67      2827.329  1.922360e+02
## 68      4267.857  4.910714e+02
## 69      4042.857  4.000000e+02
## 70      4075.776  3.307453e+02
## 71      4780.000  2.671429e+02
## 72      3492.857  2.821429e+02
## 73      3166.667  7.500000e+02
## 74      5007.143  1.750000e+02
## 75      4366.071  4.258929e+02
## 76      4082.143  4.553571e+02
## 77      5293.333  1.128571e+02
## 78      4167.857  5.410714e+02
## 79      3850.000  3.821429e+02
## 80      3482.143  3.053571e+02
## 81      4442.857  2.642857e+02
## 82      5141.071  4.491071e+02
## 83      3717.857  3.660714e+02
## 84      4157.143  4.571429e+02
## 85      3857.143  4.142857e+02
## 86      4216.071  4.401786e+02
## 87      2626.786  4.633929e+02
## 88      3307.143  2.607143e+02
## 89      5313.333  5.100000e+02
## 90      5406.667  3.585714e+02
## 91      4232.143  4.446429e+02
## 92      3771.429  5.057143e+02
## 93      4289.286  2.232143e+02
## 94      3500.000  1.500000e+03
## 96      4142.857  3.785714e+02
## 97      4600.000  3.857143e+02
## 98      4467.857  3.982143e+02
## 99      3703.571  4.589286e+02
## 100     3619.048  5.928571e+02
## 101     4709.524  6.214286e+02
## 102     4225.000  3.196429e+02
## 103     4950.000  4.250000e+02
## 104     3350.000  4.678571e+02
## 105     4180.000  5.150000e+02
## 106     3935.714  4.607143e+02
## 107     4053.571  3.125000e+02
## 108     4335.714  4.714286e+02
## 109     3782.143  4.053571e+02
## 110     3939.286  3.767857e+02
## 111     4682.143  2.696429e+02
## 112     3960.000  5.800000e+02
## 113     5021.429  3.892857e+02
## 114     3975.000  2.732143e+02
## 115     3742.857  2.785714e+02
## 116     3020.000  3.300000e+02
## 117     4491.071  6.026786e+02
## 118     3924.224  4.192547e+02
## 119     3750.000  5.750000e+02
## 120     3423.810  4.085714e+02
## 121     4932.143  4.410714e+02
## 122     4592.857  4.678571e+02
## 123     4937.838  4.959459e+02
## 124     5437.500  5.473214e+02
## 125     3500.000  9.500000e+02
## 126     4680.357  2.616071e+02
## 127     4446.429  4.017857e+02
## 128     3600.000  4.500000e+02
## 129     3852.381  4.428571e+02
## 130     3535.714  4.178571e+02
## 131     4589.286  5.875000e+02
## 132     4628.571  2.357143e+02
## 133     4489.286  3.232143e+02
## 134     5407.143  1.964286e+02
## 135     3680.952  7.071429e+02
## 136     4378.571  4.107143e+02
## 137     4964.286  3.250000e+02
## 138     3564.286  4.535714e+02
## 139     4366.667 -2.009718e-13
## 140     4792.857  3.750000e+02
## 141     5007.143  3.964286e+02
## 142     3567.857  2.339286e+02
## 143     4878.571  6.178571e+02
## 144     4528.571  5.285714e+02
## 145     4362.500  3.955357e+02
## 146     4382.143  3.267857e+02
## 147     5100.000  2.357143e+02
## 148     4617.857  3.732143e+02
## 149     5039.286  3.267857e+02
## 150     4621.429  4.821429e+02
## 151     3500.000  1.000000e+03
## 152     5196.429  3.839286e+02
## 153     4460.714  3.303571e+02
## 154     2814.286  2.250000e+02
## 155     4996.429  4.125000e+02
## 156     5028.571  4.000000e+02
## 157     4839.286  3.053571e+02
## 158     5261.905  3.142857e+02
## 159     4803.571  5.232143e+02
## 160     3950.000  6.250000e+02
## 161     5000.000  6.100000e+02
## 162     3910.714  3.910714e+02
## 164     2980.000  1.250000e+02
## 165     4319.048  2.528571e+02
## 166     3664.286  4.321429e+02
## 167     4221.429  5.107143e+02
## 168     3310.714  3.839286e+02
## 169     5430.357  3.651786e+02
## 170     4753.571  4.267857e+02
## 171     3314.286  3.857143e+02
## 173     3875.000  3.589286e+02
## 174     3890.476  2.485714e+02
## 175     4480.000  8.350000e+02
## 177     5357.143  5.500000e+02
## 178     5217.857  2.803571e+02
## 179     5625.000  4.339286e+02
## 180     4823.810  4.785714e+02
## 181     4771.429  4.428571e+02
## 182     1866.667  6.500000e+02
## 183     2278.571  2.392857e+02
## 184     5642.857  4.642857e+02
## 185     3539.286  5.053571e+02
## 186     3785.714  4.214286e+02
## 187     3817.857  5.017857e+02
## 188     4267.857  5.196429e+02
## 189     3200.000  6.500000e+02
## 190     4753.571  4.196429e+02
## 191     3816.071  4.794643e+02
## 192     4439.286  4.339286e+02
## 193     4228.571  1.928571e+02
## 194     3703.571  4.946429e+02
## 195     4528.571  5.214286e+02
## 196     4123.214  4.687500e+02
## 197     4160.714  4.767857e+02
## 198     3957.143  4.642857e+02
## 199     4860.714  3.660714e+02
## 200     4046.429  3.660714e+02
## 201     3470.000  4.600000e+02
## 202     2700.000            NA
## 203     3785.714  4.642857e+02
## 204     3471.429  3.714286e+02
## 205     2976.786  3.241071e+02
## 206     4550.000  4.178571e+02
## 207     3660.000  5.600000e+02
## 208     4612.500  4.776786e+02
## 209     4471.429  3.357143e+02
## 210     4242.857  3.785714e+02
## 211     4969.643  4.848214e+02
## 212     3485.714  4.785714e+02
## 213     3500.000  1.000000e+03
## 214     3817.857  5.017857e+02
## 215     4021.429  4.321429e+02
## 216     3760.000  6.050000e+02
## 217     4367.857  5.125000e+02
## 218     5420.000  1.257143e+02
## 219     4725.000  4.553571e+02
## 220     4517.857  4.375000e+02
## 221     3905.357  4.026786e+02
## 222     3400.000  8.000000e+02
## 223     4390.000  3.650000e+02
## 224     4605.357  4.455357e+02
## 225     3216.071  5.223214e+02
## 226     3900.000  5.150000e+02
## 227     4817.857  5.017857e+02
## 228     5055.357  4.919643e+02
## 229     4851.786  3.187500e+02
## 230     2835.714  5.285714e+02
## 231     4367.857  4.410714e+02
## 232     5083.929  5.562500e+02
## 233     3628.571  5.464286e+02
## 234     3800.000  1.100000e+03
## 235     4953.571  3.053571e+02
## 236     3755.357  3.955357e+02
## 237     4080.357  2.901786e+02
## 238     3571.429  4.107143e+02
## 239     3000.000  1.000000e+03
## 240     4400.000  8.500000e+02
## 241     4664.286  4.250000e+02
## 242     4607.143  1.035714e+02
## 243     4292.857  4.750000e+02
## 244     2567.857  5.839286e+02
## 245     3821.429  4.035714e+02
## 246     3500.000  1.300000e+03
## 247     4357.143  3.357143e+02
## 248     3871.429  5.000000e+02
## 249     3310.714  3.767857e+02
## 250     3157.143  4.000000e+02
## 251     3200.000  1.000000e+03
## 252     4107.143  3.464286e+02
## 253     3957.627  5.483051e+02
## 254     3733.333  1.928571e+02
## 255     3642.857  5.357143e+02
## 256     4242.857  3.928571e+02
## 257     3782.143  4.982143e+02
## 258     2567.857  2.553571e+02
## 259     4250.000  3.821429e+02
## 260     3200.000  4.857143e+02
## 261     3692.857  2.892857e+02
## 262     4167.857  4.625000e+02
## 263     3796.429  3.553571e+02
## 264     3450.000  3.678571e+02
## 265     4717.857  3.875000e+02
## 266     4367.857  4.053571e+02
## 267     4182.143  3.410714e+02
## 268     4464.286  4.535714e+02
## 269     4492.857  2.464286e+02
## 270     4482.143  4.982143e+02
## 271     3310.000  8.300000e+02
## 272     4725.000  3.553571e+02
## 273     5806.667  3.714286e+01
## 274     3978.571  2.821429e+02
## 275     4507.143  3.535714e+02
## 276     3917.857  3.660714e+02
## 277     4100.000  8.000000e+02
## 278     4042.857  1.857143e+02
## 279     3575.000  3.017857e+02
## 280     4117.857  5.375000e+02
## 281     4314.286  5.328571e+02
## 282     3639.286  4.196429e+02
## 283     4350.000  5.250000e+02
## 284     3721.429  5.035714e+02
## 285     4525.000  4.482143e+02
## 286     3989.286  4.232143e+02
## 287     4132.143  5.446429e+02
## 288     4542.857  1.214286e+02
## 289     3721.429  3.964286e+02
## 290     4153.571  3.839286e+02
## 291     3253.571  2.267857e+02
## 292     4212.500  4.348214e+02
## 293     4864.286  4.892857e+02
## 294     3287.500  3.508929e+02
## 295     3973.214  3.794643e+02
## 296     4687.500  3.294643e+02
## 297     4375.000  5.017857e+02
## 298     4100.000  4.900000e+02
## 299     3937.500  3.973214e+02
## 300     4482.143  4.553571e+02
## 301     4219.643  4.526786e+02
## 302     3985.714  2.928571e+02
## 303     3201.786  5.508929e+02
## 304     4483.929  5.705357e+02
## 305     5513.333  2.242857e+02
## 306     3790.000  4.950000e+02
## 307     4003.571  4.160714e+02
## 308     1986.786  4.433929e+02
## 309     3528.571  2.242857e+02
## 310     4500.000  5.428571e+02
## 311     3489.286  4.232143e+02
## 312     3250.000  5.678571e+02
## 313     4067.857  5.125000e+02
## 314     4357.143  4.071429e+02
## 315     2000.000            NA
## 316     4457.143  4.428571e+02
## 317     4421.739  4.913043e+02
## 318     4139.655  4.077586e+02
## 319     4107.143  2.392857e+02
## 320     1400.000            NA
## 321     3614.286  5.271429e+02
## 322     4539.286  4.196429e+02
## 323     4446.584  4.242236e+02
## 324     5107.143  4.678571e+02
## 325     4166.667  4.800000e+02
## 326     3564.286  4.528571e+02
## 327     5064.286  4.678571e+02
## 328     3400.000  1.400000e+03
## 329     4582.143  4.339286e+02
## 330     3707.453  5.198758e+02
## 331     3440.373  2.909938e+02
## 332     3000.000  1.000000e+03
## 333     4595.238  2.742857e+02
## 334     6008.696  6.565217e+02
## 335     4074.534  4.487578e+02
## 336     4190.476  4.685714e+02
## 337     5182.143  1.928571e+02
## 338     3954.717  3.646226e+02
## 339     3100.000  4.100000e+02
## 340     4417.857  3.517857e+02
## 341     3800.000            NA
## 342     3300.000            NA
## 343     4803.571  5.160714e+02
## 344     5486.667  2.150000e+02
## 345     3750.000  5.000000e+02
## 346     3300.000            NA
## 347     3421.429  6.042857e+02
## 348     5014.286  3.328571e+02
## 349     4967.857  5.839286e+02
## 350     3678.571  3.535714e+02
## 351     4535.714  2.107143e+02
## 352     3960.714  3.732143e+02
## 353     3080.000  3.650000e+02
## 354     5378.571  3.500000e+02
## 355     4310.714  1.767857e+02
## 356     3693.333  3.200000e+02
## 357     2517.857  5.017857e+02
## 358     4310.714  3.625000e+02
## 359     3914.035  4.903509e+02
## 360     3742.857  3.357143e+02
## 361     4635.714  3.750000e+02
## 362     4380.952  2.771429e+02
## 363     4003.571  3.160714e+02
## 364     3635.714  2.392857e+02
## 365     3525.000  2.696429e+02
## 366     4510.714  2.625000e+02
## 367     3000.000            NA
## 368     4583.333  3.178571e+02
## 369     4432.143  3.732143e+02
## 370     4700.000  2.857143e+02
## 371     4713.095  2.839286e+02
## 372     3200.000  6.500000e+02
## 373     4200.000  4.071429e+02
## 374     4269.643  2.919643e+02
## 375     3900.000  5.750000e+02
## 376     3292.857  4.607143e+02
## 377     3200.000  3.500000e+02
## 378     4203.571  4.375000e+02
## 379     5364.286  4.678571e+02
## 380     5096.429  2.982143e+02
## 381     4189.286  5.517857e+02
## 382     5335.714  5.535714e+02
## 383     4750.000  4.607143e+02
## 384     3425.000  4.839286e+02
## 385     3100.000  2.500000e+02
## 386     4596.429  3.910714e+02
## 387     3628.571  3.642857e+02
## 388     3471.429  4.357143e+02
## 389     3057.143  3.285714e+02
## 390     3210.714  3.196429e+02
## 391     4364.286  4.178571e+02
## 392     4353.571  5.196429e+02
## 393     2500.000  2.500000e+02
## 394     3825.000  4.196429e+02
## 395     3666.667  7.500000e+02
## 396     3653.571  5.125000e+02
## 397     4446.429  5.803571e+02
## 398     4600.000  3.142857e+02
## 399     3604.762  1.157143e+02
## 400     3521.429  4.250000e+02
## 401     2300.000  5.000000e+02
## 402     3721.429  5.892857e+02
## 403     4253.571  4.982143e+02
## 404     3810.714  3.410714e+02
## 405     4689.286  5.089286e+02
## 406     4182.143  4.767857e+02
## 407     3335.714  5.392857e+02
## 408     4603.571  5.089286e+02
## 409     2300.000  3.215549e-13
## 410     2807.143  4.678571e+02
## 411     4057.143  3.857143e+02
## 412     3675.000  4.303571e+02
## 413     3439.286  5.053571e+02
## 414     4350.000  3.821429e+02
## 415     3000.000            NA
## 416     3810.714  4.482143e+02
## 417     3195.238  4.942857e+02
## 418     4420.000  2.100000e+02
## 419     4260.714  2.946429e+02
## 420     3535.714  4.892857e+02
## 421     3821.429  4.250000e+02
## 422     4046.429  3.660714e+02
## 423     4107.143  3.607143e+02
## 424     2420.000  2.700000e+02
## 425     3332.143  2.803571e+02
## 426     4089.286  5.803571e+02
## 427     4557.143  4.071429e+02
## 428     4157.143  3.714286e+02
## 429     3864.286  4.464286e+02
## 430     3085.714  4.285714e+02
## 431     3857.143  3.357143e+02
## 432     3371.429  4.642857e+02
## 433     4653.571  3.839286e+02
## 434     5023.810  6.035714e+02
## 435     4682.143  2.696429e+02
## 436     4571.429  4.214286e+02
## 437     4957.143  3.214286e+02
## 438     3000.000  1.550000e+03
## 439     3659.627  3.012422e+02
## 440     3600.000  4.800000e+02
## 441     3432.143  3.875000e+02
## 442     4792.857  3.464286e+02
## 443     4767.857  2.767857e+02
## 444     4952.174  3.760870e+02
## 445     4500.000  3.571429e+02
## 446     3267.857  4.339286e+02
## 447     1600.000            NA
## 448     4671.429  3.857143e+02
## 449     3121.429  2.035714e+02
## 450     4642.857  3.642857e+02
## 451     3728.571  5.785714e+02
## 452     4035.714  3.392857e+02
## 453     4578.571  3.250000e+02
## 454     3885.714  4.000000e+02
## 455     4200.000  3.357143e+02
## 456     3860.000  3.500000e+02
## 457     3203.571  5.446429e+02
## 458     3000.000            NA
## 459     4064.286  4.821429e+02
## 460     3978.571  3.535714e+02
## 461     4300.000  8.500000e+02
## 462     4732.143  3.517857e+02
## 463     4053.571  6.410714e+02
## 464     3810.714  4.553571e+02
## 465     3867.857  3.410714e+02
## 466     3500.000  9.000000e+02
## 467     4000.000  3.214286e+02
## 468     5664.286  4.035714e+02
## 469     3942.857  3.357143e+02
## 470     4578.571  2.821429e+02
## 471     4133.333  5.500000e+02
## 472     4650.000  4.535714e+02
## 473     4320.000  3.050000e+02
## 474     3680.000  4.400000e+02
## 475     2571.429  4.107143e+02
## 476     4546.429  4.017857e+02
## 477     3802.381  4.357143e+02
## 478     2500.000            NA
## 479     3191.071  4.598214e+02
## 480     4051.190  5.803571e+02
## 481     3466.667  5.600000e+02
## 482     3100.000 -2.500000e+02
## 483     3847.857  3.707143e+02
## 484     3646.429  4.017857e+02
## 485     3850.000  4.250000e+02
## 486     4096.429  4.125000e+02
## 487     4040.000  5.700000e+02
## 488     4396.429  3.696429e+02
## 489     3171.429  2.928571e+02
## 490     4564.286  2.035714e+02
## 491     4096.429  4.196429e+02
## 492     2800.000  1.000000e+02
## 493     4017.857  5.232143e+02
## 494     4276.786  3.598214e+02
## 495     3453.571  3.839286e+02
## 496     4717.143  2.342857e+02
## 497     5585.714  4.142857e+02
## 498     4237.500  4.223214e+02
## 499     4964.286  4.464286e+02
## 500     3014.286  3.642857e+02
## 501     3967.857  3.839286e+02
## 502     3483.333  7.750000e+02
## 503     4592.857  5.321429e+02
## 504     4260.714  3.089286e+02
## 505     4232.143  3.946429e+02
## 506     4403.571  5.660714e+02
## 507     5189.286  3.232143e+02
## 508     4539.286  3.910714e+02
## 509     3189.286  4.946429e+02
## 510     4167.857  5.267857e+02
## 511     4757.143  3.785714e+02
## 512     3600.000  7.000000e+02
## 513     3500.000  8.500000e+02
## 514     3421.429  4.714286e+02
## 515     3840.476  4.821429e+02
## 516     3946.429  3.660714e+02
## 517     3683.333  4.750000e+02
## 518     3554.310  5.780172e+02
## 519     4435.714  3.750000e+02
## 520     4061.491  4.639752e+02
## 521     4221.429  2.464286e+02
## 522     5082.143  7.196429e+02
## 523     4321.429  3.928571e+02
## 524     4034.524  2.303571e+02
## 525     4120.238  3.160714e+02
## 526     4085.714  3.000000e+02
## 527     3575.000  3.375000e+02
## 528     3850.000  9.250000e+02
## 529     4808.696  3.565217e+02
## 530     4907.143  2.892857e+02
## 531     4294.410  3.350932e+02
## 532     4292.857  4.178571e+02
## 533     4303.571  2.875000e+02
## 534     3620.000  6.150000e+02
## 535     3280.000  4.414286e+02
## 536     4303.571  6.517857e+02
## 537     4053.571  4.339286e+02
## 538     4642.857  4.071429e+02
## 539     4628.571  4.642857e+02
## 540     3596.429  3.839286e+02
## 541     3982.143  2.732143e+02
## 542     3896.429  3.625000e+02
## 543     3878.571  3.321429e+02
## 544     4200.000  4.428571e+02
## 545     4561.194  4.783582e+02
## 546     4317.857  2.875000e+02
## 547     2700.000            NA
## 548     3600.000            NA
## 549     4889.286  3.303571e+02
## 550     4410.714  4.339286e+02
## 551     4057.143  3.857143e+02
## 552     4357.143  3.785714e+02
## 553     3300.000  7.642857e+02
## 554     3591.429  3.328571e+02
## 555     3953.571  2.625000e+02
## 556     4082.143  4.196429e+02
## 557     3907.143  3.678571e+02
## 558     4185.714  3.214286e+02
## 559     4378.571  4.178571e+02
## 560     2500.000            NA
## 561     3996.429  5.125000e+02
## 562     3300.000            NA
## 563     3680.000  4.400000e+02
## 564     3278.571  3.607143e+02
## 565     3000.000  1.000000e+03
## 566     3701.190  5.053571e+02
## 567     4882.143  4.839286e+02
## 568     4021.429  3.035714e+02
## 569     4301.786  3.937500e+02
## 570     5435.714  4.321429e+02
## 571     4907.143  3.178571e+02
## 572     4417.857  3.875000e+02
## 573     3767.857  2.982143e+02
## 574     3996.429  5.553571e+02
## 575     5239.286  3.696429e+02
## 576     4678.571  3.464286e+02
## 577     3140.000  5.450000e+02
## 578     4448.447  3.885093e+02
## 579     4514.286  3.928571e+02
## 580     3425.000  3.839286e+02
## 581     2700.000  9.000000e+02
## 582     3825.000  5.625000e+02
## 583     3770.270  6.317568e+02
## 584     3742.857  4.000000e+02
## 585     3300.000            NA
## 586     2800.000            NA
## 587     3666.667  6.500000e+02
## 588     4221.429  3.392857e+02
## 589     3880.000  5.050000e+02
## 590     4717.857  5.875000e+02
## 591     5692.857  3.821429e+02
## 592     3996.429  4.339286e+02
## 593     5025.000  3.553571e+02
## 594     3996.429  3.125000e+02
## 595     3966.071  4.544643e+02
## 596     6117.857  4.303571e+02
## 597     4807.143  2.821429e+02
## 598     4414.286  4.000000e+02
## 599     3000.000            NA
## 600     3146.429  4.875000e+02
## 601     3985.075  4.835821e+02
## 602     3500.000  1.050000e+03
## 603     4489.286  4.875000e+02
## 604     4803.571  4.089286e+02
## 605     3194.643  3.044643e+02
## 606     5163.690  4.491071e+02
## 607     5189.286  3.303571e+02
## 608     2650.000  7.250000e+02
## 609     3283.333  8.250000e+02
## 610     4189.286  3.517857e+02
## 611     4761.905  5.442857e+02
## 612     3653.571  3.910714e+02
## 613     3826.786  4.098214e+02
## 614     4535.714  4.000000e+02
## 615     3421.429  3.178571e+02
## 616     3982.143  5.267857e+02
## 617     4942.857  4.892857e+02
## 618     3670.000  8.350000e+02
## 619     4535.714  4.142857e+02
## 620     3635.714  5.000000e+02
## 621     3328.571  4.285714e+02
## 622     3880.952  4.671429e+02
## 623     4975.000  4.303571e+02
## 624     4737.500  2.937500e+02
## 625     4000.000  1.000000e+03
## 626     4369.643  3.312500e+02
## 627     3540.000  5.500000e+02
## 628     3600.000  5.000000e+01
## 629     5037.500  4.401786e+02
## 630     5448.649  2.986486e+02
## 631     3796.429  5.053571e+02
## 632     3696.429  3.696429e+02
## 633     3551.786  4.294643e+02
## 634     4982.143  3.160714e+02
## 635     4576.786  3.705357e+02
## 636     4498.214  4.455357e+02
## 637     4350.000  4.107143e+02
## 638     4339.286  5.125000e+02
## 639     3921.429  4.178571e+02
## 640     4185.714  4.285714e+02
## 641     4391.071  3.205357e+02
## 642     4151.786  3.116071e+02
## 643     4453.571  2.375000e+02
## 644     4894.643  4.223214e+02
## 645     3064.286  5.964286e+02
## 646     4435.714  3.607143e+02
## 647     4678.571  5.178571e+02
## 648     4225.000  4.267857e+02
## 649     2750.000  4.464286e+02
## 650     3600.000  3.964286e+02
## 651     4171.429  5.035714e+02
## 652     4189.286  3.410714e+02
## 653     5839.286  5.267857e+02
## 654     4171.429  4.642857e+02
## 655     3591.071  4.526786e+02
## 656     4582.143  5.410714e+02
## 657     4675.000  5.517857e+02
## 658     4821.429  3.750000e+02
## 659     5210.714  5.767857e+02
## 660     3846.429  3.017857e+02
## 661     3807.143  5.250000e+02
## 662     5164.286  5.357143e+02
## 663     3492.857  4.035714e+02
## 664     5233.929  4.098214e+02
## 665     3185.714  4.000000e+02
## 666     3900.000  4.000000e+02
## 667     3842.857  3.571429e+02
## 668     3071.429  3.857143e+02
## 669     5225.000  4.053571e+02
## 670     4716.071  4.651786e+02
## 671     4721.429  4.321429e+02
## 672     3846.429  2.660714e+02
## 673     4830.357  2.580357e+02
## 674     4128.571  4.000000e+02
## 675     3875.000  4.160714e+02
## 676     3821.429  3.321429e+02
## 677     2700.000  1.150000e+03
## 678     4235.714  3.071429e+02
## 679     3685.714  3.214286e+02
## 680     3825.000  9.107143e+01
## 681     3953.571  1.767857e+02
## 682     3000.000            NA
## 683     4339.286  3.767857e+02
## 684     5103.571  3.589286e+02
## 685     3332.143  1.589286e+02
## 686     4617.857  4.303571e+02
## 687     4275.000  4.089286e+02
## 688     3471.429  3.057143e+02
## 689     3621.429  4.250000e+02
## 690     4078.571  2.607143e+02
## 691     4042.857  4.500000e+02
## 692     3985.714  4.785714e+02
## 693     2916.667  6.250000e+02
## 694     4507.143  6.321429e+02
## 695     3971.429  3.142857e+02
## 696     4503.571  3.803571e+02
## 697     3583.333  7.750000e+02
## 698     5126.667  4.771429e+02
## 699     4357.143  4.285714e+02
## 700     3771.429  3.071429e+02
## 701     3985.714  4.000000e+02
## 702     3800.000  9.000000e+02
## 703     3570.000  4.350000e+02
## 704     4553.571  3.196429e+02
## 705     3885.714  3.428571e+02
## 706     6078.571  4.464286e+02
## 707     3648.214  4.312500e+02
## 708     2700.000            NA
## 709     4117.857  4.089286e+02
## 710     4689.286  3.660714e+02
## 711     3982.143  3.196429e+02
## 712     3000.000  6.000000e+02
## 713     4200.000  4.428571e+02
## 714     4239.286  3.482143e+02
## 715     3500.000            NA
## 716     4703.571  3.446429e+02
## 717     3200.000            NA
## 718     3421.429  3.750000e+02
## 719     4050.000  3.250000e+02
## 720     3625.000  3.625000e+02
## 721     4232.143  4.089286e+02
## 722     4553.571  5.053571e+02
## 723     4253.571  2.625000e+02
## 724     5266.667  2.571429e+02
## 725     4271.429  3.071429e+02
## 726     3335.714  2.892857e+02
## 727     3182.143  2.625000e+02
## 728     3007.143  2.464286e+02
## 729     2780.000  1.750000e+02
## 730     3500.000 -4.500000e+02
## 731     4417.857  4.017857e+02
## 732     3520.000  5.900000e+02
## 733     3300.000  6.500000e+02
## 734     4325.000  3.125000e+02
## 735     4771.429  2.214286e+02
## 736     3767.857  4.803571e+02
## 737     4525.000  2.910714e+02
## 738     3421.429  3.750000e+02
## 739     4335.714  4.464286e+02
## 740     4728.571  3.214286e+02
## 741     4807.143  3.535714e+02
## 742     3703.571  4.089286e+02
## 743     3664.286  4.821429e+02
## 744     4482.143  4.125000e+02
## 745     4439.286  4.696429e+02
## 746     3485.714  3.528571e+02
## 747     4921.429  7.535714e+02
## 748     4535.714  5.607143e+02
## 749     4816.071  2.437500e+02
## 750     4214.286  4.071429e+02
## 751     3728.571  4.785714e+02
## 752     4040.000  2.171429e+02
## 753     3789.286  4.517857e+02
## 754     3844.643  4.366071e+02
## 755     3996.429  4.553571e+02
## 756     4807.143  4.464286e+02
## 757     1700.000            NA
## 758     4875.000  2.517857e+02
## 759     3946.429  5.017857e+02
## 760     4414.286  3.178571e+02
## 761     3817.857  3.767857e+02
## 762     3937.500  2.901786e+02
## 763     3900.000  3.142857e+02
## 764     4382.143  5.053571e+02
## 765     2990.000  3.350000e+02
## 766     3000.000  1.000000e+03
## 767     3426.786  3.241071e+02
## 768     3300.000            NA
## 769     3100.000            NA
## 770     4446.429  3.910714e+02
## 771     5407.143  3.000000e+02
## 772     4350.000  3.250000e+02
## 773     4005.357  5.241071e+02
## 774     4910.714  3.660714e+02
## 775     4058.929  3.830357e+02
## 776     4478.571  4.357143e+02
## 777     4058.929  3.008929e+02
## 778     3821.429  4.321429e+02
## 779     4083.333  5.750000e+02
## 780     3200.000  5.000000e+02
## 781     4814.286  3.714286e+02
## 782     3564.286  5.250000e+02
## 783     3764.286  3.892857e+02
## 784     3000.000            NA
## 785     3710.714  4.803571e+02
## 786     2760.000  5.550000e+02
## 787     3714.286  3.142857e+02
## 788     3000.000            NA
## 789     3792.857  4.321429e+02
## 790     3280.000  7.400000e+02
## 791     3573.292  4.723602e+02
## 792     4060.714  2.017857e+02
## 793     4689.286  4.303571e+02
## 794     4067.857  4.053571e+02
## 795     4560.714  3.589286e+02
## 796     3813.333  4.171429e+02
## 797     4325.000  3.839286e+02
## 798     2500.000  1.050000e+03
## 799     4303.571  3.660714e+02
## 800     4007.143  4.035714e+02
## 801     4060.714  4.732143e+02
## 802     2389.286  4.875000e+02
## 803     3700.000            NA
## 804     4782.143  4.625000e+02
## 805     3700.000  2.928571e+02
## 806     4214.286  1.035714e+02
## 807     2916.667  1.125000e+03
## 808     3250.000  8.750000e+02
## 809     3540.476  4.321429e+02
## 810     3840.000  6.700000e+02
## 811     3714.286  8.714286e+01
## 812     4300.000  4.285714e+02
## 813     4528.571  4.642857e+02
## 814     4514.286  4.500000e+02
## 815     4328.571  3.857143e+02
## 816     4675.000  3.732143e+02
## 817     3000.000  3.215549e-13
## 818     2435.714  5.464286e+02
## 819     3500.000            NA
## 820     4292.857  4.321429e+02
## 821     5253.571  3.696429e+02
## 822     3925.000  3.125000e+02
## 823     4289.286  4.303571e+02
## 824     3403.571  1.946429e+02
## 825     4235.714  3.464286e+02
## 826     4240.000  3.450000e+02
## 827     4160.000  4.150000e+02
## 828     3767.857  2.267857e+02
## 829     3896.429  2.410714e+02
## 830     2939.286  3.267857e+02
## 831     2816.667  7.750000e+02
## 832     3603.571  4.089286e+02
## 833     3183.333  9.250000e+02
## 834     4196.429  3.982143e+02
## 835     3470.000  6.600000e+02
## 836     3821.429  4.107143e+02
## 837     4057.143  2.928571e+02
## 838     4554.762  4.464286e+02
## 839     4050.000  3.892857e+02
## 840     4058.385  2.220497e+02
## 841     5696.429  4.910714e+02
## 842     3171.429  2.928571e+02
## 843     3330.000  6.650000e+02
## 844     3471.429  4.857143e+02
## 845     4403.571  4.446429e+02
## 846     3925.000  2.910714e+02
## 847     3300.000            NA
## 848     4017.857  3.660714e+02
## 849     3967.857  5.839286e+02
## 850     4763.095  2.589286e+02
## 851     4383.333  1.525000e+03
## 852     4432.143  3.232143e+02
## 853     4264.286  3.607143e+02
## 854     4440.000  9.200000e+02
## 855     3100.000  3.000000e+02
## 856     4835.714  3.464286e+02
## 857     6453.571  4.839286e+02
## 858     4835.714  3.035714e+02
## 859     4517.857  3.089286e+02
## 860     4307.143  3.035714e+02
## 861     3614.286  2.714286e+02
## 862     4560.000  3.800000e+02
## 863     4635.714  3.750000e+02
## 864     4239.286  2.125000e+02
## 865     3960.000  7.225000e+02
## 866     4046.429  3.375000e+02
## 867     3600.000  1.150000e+03
## 868     4110.714  3.767857e+02
## 869     4464.286  4.607143e+02
## 870     4064.286  3.392857e+02
## 871     4600.000  2.000000e+02
## 872     4810.714  1.267857e+02
## 873     4478.571  2.750000e+02
## 874     3942.857  3.414286e+02
## 875     3814.286  3.857143e+02
## 876     3739.286  2.696429e+02
## 877     4092.857  4.464286e+02
## 878     4842.857  4.142857e+02
## 879     4171.429  3.857143e+02
## 880     4475.000  3.160714e+02
## 881     4200.000  3.357143e+02
## 882     4876.786  4.741071e+02
## 883     3878.571  3.392857e+02
## 884     4400.000  3.857143e+02
## 885     4060.714  4.517857e+02
## 886     2950.000  8.750000e+02
## 887     5300.000  3.857143e+02
## 888     6235.714  4.750000e+02
## 889     3578.571  2.821429e+02
## 890     4089.286  3.803571e+02
## 891     3500.000  4.500000e+02
## 892     2800.000  9.000000e+02
## 893     5132.143  2.303571e+02
## 894     3000.000  3.215549e-13
## 895     3500.000  3.000000e+02
## 896     4607.143  4.178571e+02
## 897     4703.571  2.803571e+02
## 898     5103.571  4.089286e+02
## 899     4021.429  3.392857e+02
## 900     4696.429  4.410714e+02
## 901     2900.000  1.050000e+03
## 902     4467.857  4.125000e+02
## 903     4464.286  3.392857e+02
## 904     4421.429  1.250000e+02
## 905     3864.286  2.821429e+02
## 906     3731.677  5.844720e+02
## 907     5271.429  4.500000e+02
## 908     3964.286  4.535714e+02
## 909     4371.429  3.214286e+02
## 910     4360.714  3.839286e+02
## 911     4025.000  1.767857e+02
## 912     3946.429  3.875000e+02
## 913     5314.286  3.000000e+02
## 914     4146.429  4.589286e+02
## 915     4060.714  3.089286e+02
## 916     3346.429  3.946429e+02
## 917     3717.857  3.446429e+02
## 918     4314.286  4.357143e+02
## 919     4460.714  3.589286e+02
## 920     4280.952  4.571429e+02
## 921     3973.214  4.437500e+02
## 922     5103.571  3.267857e+02
## 923     4548.214  5.741071e+02
## 924     4053.571  4.196429e+02
## 925     3166.667  1.000000e+03
## 926     3039.286  3.125000e+02
## 927     4753.571  2.982143e+02
## 928     4517.857  5.089286e+02
## 929     4128.571  3.892857e+02
## 930     3000.000            NA
## 931     3739.286  4.982143e+02
## 932     3780.000  5.650000e+02
## 933     5003.571  5.660714e+02
## 934     4317.857  3.196429e+02
## 935     3767.857  3.839286e+02
## 936     3400.000  1.300000e+03
## 937     4267.857  4.196429e+02
## 938     4482.143  3.839286e+02
## 939     4671.429  4.321429e+02
## 940     2833.929  1.955357e+02
## 941     3664.286  3.035714e+02
## 942     4642.857  4.714286e+02
## 943     4314.286  4.428571e+02
## 944     3241.071  3.312500e+02
## 945     2500.000  1.000000e+03
## 946     4728.571  2.428571e+02
## 947     5225.000  2.839286e+02
## 948     3650.000  5.750000e+02
## 949     4042.857  4.392857e+02
## 950     4220.000  5.900000e+02
## 951     5789.286  3.303571e+02
## 952     3546.429  3.732143e+02
## 953     4167.857  4.625000e+02
## 954     4095.238  3.592857e+02
## 955     4300.000  4.000000e+02
## 956     3089.286  2.089286e+02
## 957     3903.571  4.017857e+02
## 958     4957.143  4.035714e+02
## 959     4842.857  5.392857e+02
## 960     4428.571  4.000000e+02
## 961     4439.286  4.910714e+02
## 962     4080.357  3.973214e+02
## 963     3857.143  4.285714e+02
## 964     5257.143  3.000000e+02
## 965     3500.000            NA
## 966     4483.929  3.883929e+02
## 967     3500.000  1.750000e+03
## 968     5767.857  3.375000e+02
## 969     4667.857  2.767857e+02
## 970     5575.000  5.089286e+02
## 971     3750.000  2.892857e+02
## 972     3000.000  1.000000e+03
## 973     4732.143  2.839286e+02
## 974     5428.571  3.214286e+02
## 975     4676.786  4.276786e+02
## 976     4753.571  4.875000e+02
## 977     3832.143  5.196429e+02
## 978     3978.571  3.607143e+02
## 979     3100.000  1.250000e+03
## 980     5010.714  4.089286e+02
## 981     4592.857  6.892857e+02
## 982     3537.500  4.758929e+02
## 983     4928.571  2.285714e+02
## 984     3937.500  4.616071e+02
## 985     4719.643  3.883929e+02
## 986     4217.857  4.446429e+02
## 987     3600.000  1.300000e+03
## 988     4021.429  3.892857e+02
## 989     3500.000  1.000000e+03
## 990     4246.429  4.089286e+02
## 991     4175.000  3.982143e+02
## 992     4912.500  2.991071e+02
## 993     3450.000  3.750000e+01
## 994     5139.286  4.339286e+02
## 995     4350.000  3.892857e+02
## 996     4389.286  3.410714e+02
## 997     3075.000  1.625000e+02
## 998     4866.071  4.473214e+02
## 999     5344.643  2.794643e+02
## 1000    2100.000            NA
## 
## Degrees of freedom: 6099 total; 4119 residual
## Residual standard error: 764.8377
```
The formula weight ~ age | ind specifies that we want to model the weight of infants as a function of age, with random intercepts for each individual (ind). This allows us to account for individual variation in weight trajectories over time. 
---

#### Exploring the random effects

- The &lt;span style="color:blue"&gt;main purpose&lt;/span&gt; of this preliminary analysis is to give an indication
  of &lt;span style="color:blue"&gt;what random effects structure&lt;/span&gt; to use in the model.
- We must decide which random effects to include in a model for the
  data, and &lt;span style="color:blue"&gt;what covariance structure&lt;/span&gt; these random effects should have.

- Objects returned by &lt;span style="color:purple"&gt;lmList&lt;/span&gt; are of class &lt;span style="color:purple"&gt;lmList&lt;/span&gt;, for which several
  display and plot methods are available.
- The &lt;span style="color:purple"&gt;pairs method&lt;/span&gt; provides one view of the random effects covariance
  structure.
- To identify outliers-points outside the estimated probability contour
  at &lt;span style="color:red"&gt;level `\(1-\alpha/2\)`&lt;/span&gt; will be marked in the plot, we use the R function.

- We see that &lt;span style="color:red"&gt;subject 29&lt;/span&gt; has high slope.

---

#### Exploring the correlation structure

- In longitudinal data analysis we model two key components of the data:
  - &lt;span style="color:blue"&gt;Mean structure&lt;/span&gt;
  - &lt;span style="color:blue"&gt;Correlation structure&lt;/span&gt; (after removing the mean structure)

- Modelling the correlation is important to be able to obtain correct inferences on regression coefficients.

- Correlation can be formulated in terms of:
  - **&lt;span style="color:purple"&gt;Random effects&lt;/span&gt;**
  - **&lt;span style="color:purple"&gt;Autocorrelation&lt;/span&gt;** or serial dependence
  - **&lt;span style="color:purple"&gt;Noise, measurement error&lt;/span&gt;**

- After we explore the mean function in the regression, we need to explore the &lt;span style="color:red"&gt;correlation structure for the residuals&lt;/span&gt;, taking away the mean trend effect.
---

#### Observed variance for jimma dataset

- Having an appropriate model for studying the evolution of the variance is a very important step in the modeling approach.
- The observed variance shows an increase in variability over time.
- Hence, a &lt;span style="color:blue"&gt;heterogeneous variance structure&lt;/span&gt; may be a good starting point.
- Moreover, the variability for &lt;span style="color:red"&gt;males and females&lt;/span&gt; seems to be more or less the same.
- Hence, the &lt;span style="color:purple"&gt;same variance structure&lt;/span&gt; may be assumed for both groups.


```r
interaction.plot(Infant$age, sex, Infant$weight, fun=var, col=2:3, xlab="age", 
                 ylab="var[wt]", las=1)
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-24-1.png" style="display: block; margin: auto;" /&gt;

---

#### &lt;span style="color:blue"&gt;Growth data&lt;/span&gt;

- The distance from the center of the pituitary to the maxillary fissure was recorded at ages &lt;span style="color:red"&gt;8, 10, 12, and 14&lt;/span&gt;, for &lt;span style="color:purple"&gt;11 girls&lt;/span&gt; and &lt;span style="color:purple"&gt;16 boys&lt;/span&gt;.

**&lt;span style="color:blue"&gt;Research Question:&lt;/span&gt;** Is dental growth related to **&lt;span style="color:red"&gt;gender&lt;/span&gt;?**

- The individual profiles support a &lt;span style="color:red"&gt;random-intercepts model&lt;/span&gt;.


```r
xyplot(measure ~ factor(age) | sex, group = ind, data = growth, 
       main="Individual profile plot by sex",xlab = "Age", ylab = "measure", 
       type = "a", lines = TRUE)
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-25-1.png" style="display: block; margin: auto;" /&gt;

---

From the exploratory analysis:
- Mean structure seems &lt;span style="color:red"&gt;linear&lt;/span&gt; over time.
- Variability between subjects at baseline.
- Variability between subjects in the way they evolve.

Hence, a linear mean with &lt;span style="color:red"&gt;random intercept and slope&lt;/span&gt; is a good idea...
---

#### &lt;span style="color:blue"&gt;Exploring the mean structure of growth data&lt;/span&gt;

For balanced data, averages can be calculated for each occasion separately, and standard errors for the means can be added.

```r
attach(growth)
mean1&lt;-tapply(measure, age, mean)
age1&lt;-sort(as.numeric(unique(age)))
plot(age1, mean1, type= "l",ylim=c(20,30), xlab="age", ylab=
" The mean distance", lwd=3, main="The mean profile")
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-26-1.png" style="display: block; margin: auto;" /&gt;

- Data exploration is therefore extremely helpful as additional tool in the selection of appropriate models.

---


```r
##### R-code for Correlation matrix ####
d1&lt;-measure[age==8]
d2&lt;-measure[age==10]
d3&lt;-measure[age==12]
d4&lt;-measure[age==14]
response1&lt;-cbind(d1, d2, d3, d4)
cor_matrix &lt;- cor(response1)
cor_matrix
```

```
##            d1         d2         d3         d4
## d1 1.00000000 0.04600947 0.71080794 0.59983380
## d2 0.04600947 1.00000000 0.06913238 0.01175495
## d3 0.71080794 0.06913238 1.00000000 0.79499798
## d4 0.59983380 0.01175495 0.79499798 1.00000000
```
Scatter plot matrix for growth data

---

```r
# Scatter plot matrix
panel.hist &lt;- function(x, ...)
{usr &lt;- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) );  h &lt;- hist(x, plot = FALSE)
  breaks &lt;- h$breaks; nB &lt;- length(breaks)
  y &lt;- h$counts; y &lt;- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)}
pairs(response1, panel=panel.smooth, cex = 1.5, pch = 16,  bg="light green",
      diag.panel=panel.hist, cex.labels = 2, font.labels=2)
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-28-1.png" style="display: block; margin: auto;" /&gt;

---


```r
pmplots::pairs_plot(response1, y = c("d1", "d2", "d3", "d4"), col = "cyan")
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-29-1.png" style="display: block; margin: auto;" /&gt;

---

#### Exploring the Variability of the Observed Data

- The individual profile plots of the growth data set exhibit substantial variability within and between subjects. 

- This intricate variability can be further elucidated by considering the variance-covariance matrix of the observed data, as indicated below:


- By examining the variance-covariance matrix, we gain deeper insights into the extensive variability present within and between subjects in the growth data set.

**Covariance Matrix for Growth Data:**


```r
cov(response1)
```

```
##           d1         d2        d3         d4
## d1 5.9259259 0.22934473 4.8753561 4.03988604
## d2 0.2293447 4.19301994 0.3988604 0.06659544
## d3 4.8753561 0.39886040 7.9387464 6.19729345
## d4 4.0398860 0.06659544 6.1972934 7.65455840
```

---

#### Exploring Overall Variability


```r
## Mean evolution profile ##
mean1&lt;-tapply(measure, age, mean)
age1&lt;-sort(as.numeric(unique(age)))
plot(sort(age1), mean1, type= "l",ylim=c(10,32), xlab="age", 
     ylab=" The mean distance", lwd=3, main=" The mean profile")
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-31-1.png" style="display: block; margin: auto;" /&gt;

---

#### Variability by Group


```r
interaction.plot(age, sex, measure, lty=c(1, 2), fun=var,
                 ylab="Distance from Pituitary to Pterygomaxillary Fissure (mm)",
                 xlab="Age", trace.label="Group")
title(main="The Variance of the Growth Data Set by Sex")
```

&lt;img src="Ldata-Analysis_files/figure-html/unnamed-chunk-32-1.png" style="display: block; margin: auto;" /&gt;

---
class: inverse, middle

# Day 2
### A Model for Longitudinal Data

- Linear Mixed Models (LMM)
- Hierarchical versus Marginal Model

- Marginal Model: Estimation and Inference

- Inference for the Random Effects

---

#### &lt;span style="color:blue"&gt;Linear Mixed Models&lt;/span&gt;

- **&lt;span style="color:purple"&gt;Linear mixed models (LMM)&lt;/span&gt;** are models that handle data where observations are not independent.

- That is, LMM correctly models correlated errors, whereas procedures in the general linear model family (GLM) usually do not.

  - (GLM includes: t-tests, ANOVA, correlation, regression, and factor analysis, to name a few.)

- LMM can be considered as a further generalization of GLM to better support the analysis of a continuous response.

- Mixed models contain both fixed and random effects.

- These models are useful in a wide variety of disciplines in the physical, biological, and economic sciences.

- They are particularly useful in settings where repeated measurements are made on the same statistical units or where measurements are made on clusters of related statistical units.

- Let us see some of the terms associated with mixed models.

---

#### Types of Effects in Linear Mixed Models

**&lt;span style="color:red"&gt; Fixed Effects&lt;/span&gt;**

- **&lt;span style="color:purple"&gt;Factors for which the only levels under consideration are contained in the coding of those effects.&lt;/span&gt;**
  - Example: **Sex** where both male and female genders are included in the factor, is considered a fixed effect.
  - Example: **Agegroup** with levels "Minor" and "Adult" included in the factor is also considered a fixed effect.

**&lt;span style="color:red"&gt; Random Effects &lt;/span&gt;**

- **&lt;span style="color:purple"&gt;Factors for which the levels contained in the coding of those factors are a random sample of the total number of levels in the population for that factor.&lt;/span&gt;**
  - Example: **Subject** can be considered a random effect if it represents a random sample of the target population.

- Random effects models allow researchers to make inferences over a wider population in Linear Mixed Models (LMM) than would be possible with Generalized Linear Models (GLM).

---

**&lt;span style="color:red"&gt; Hierarchical Effects&lt;/span&gt;**

- **&lt;span style="color:purple"&gt;Hierarchical designs have nested effects.&lt;/span&gt;**
  - Nested effects are those with subjects within groups. For instance, in a medical study, "Patients" may be nested within "Doctors," and "Doctors" may, in turn, be nested within "Hospitals."
  - We can have a hierarchical effect when the predictor variables are measured at more than one level (ex., reading achievement scores at the student level and teacher-student ratios at the school level).

- Considering hierarchical effects in Linear Mixed Models allows researchers to account for the nested structure of the data and make more accurate inferences about the relationships between variables at different levels of the hierarchy.

---

#### In Practice: Handling Unbalanced Data

- Often, data is unbalanced:
  - Unequal number of measurements per subject
  - Measurements not taken at fixed time points

- As a result, traditional multivariate regression techniques may not be applicable.

#### Subject-Specific Profiles

- Subject-specific longitudinal profiles can be well approximated by linear regression functions.

- A 2-stage model formulation is common:
 
  1. **Stage 1**: Linear regression model for each subject separately.
 
  2. **Stage 2**: Explaining variability in subject-specific regression coefficients using known covariates.

---

#### Stage 1 Model

`$$Y_{i} = Z_{i}\beta_{i} + \varepsilon_{i}$$`

- `\(Y_{i} = (Y_{i1}, Y_{i2}, ..., Y_{in_i})'\)`

- `\(Z_{i}\)` is a `\(n_i \times q\)` matrix of known covariates.

- `\(\beta_{i}\)` is a `\(q\)` dimensional vector of subject-specific regression coefficients.

- `\(\varepsilon_{i} \sim \color{blue}{N(0, \Sigma_{i})}\)`.

- Often, `\(\Sigma_{i} = \color{blue}{\sigma^2 I_{N_{i}}}\)`.

- This model describes the observed variability within subjects.

---

#### A 2-stage Model 

- Between-subject variability can now be studied by relating the `\(\beta_{i}\)` to known covariates.

- **Stage 2 model**: 
`$$\beta_{i} = K_{i}\beta + b_{i}$$`

  - `\(K_{i}\)` is a `\(q \times p\)` matrix of known covariates.

  - `\(\beta\)` is a `\(p\)`-dimensional vector of unknown parameters.

  - `\(b_{i} \sim \color{blue}{N(0, D)}\)`.
  
---

#### The General Linear Mixed-effects Model

- A 2-stage approach can be performed explicitly in the analysis.

- However, this is just another example of the use of summary statistics.
   - `\(Y_{i}\)` is summarized by `\(\hat{\beta}_{i}\)`.
   - Summary statistics `\(\hat{\beta}_{i}\)` are analyzed in the second stage.

- The associated drawbacks can be avoided by combining the two stages into one model:

`$$\begin{cases}
    Y_{i} = Z_{i}\beta_{i} + \varepsilon_{i} \\
    \beta_{i} = K_{i}\beta + b_{i}
\end{cases}$$`


`$$\Rightarrow Y_{i} = Z_{i}K_{i}\beta + Z_{i}b_{i} + \varepsilon_{i}$$` 
`$$= X_{i}\beta + Z_{i}b_{i} + \varepsilon_{i}$$`

---

The model is given by:

`$$Y_i = \underset{\text{fixed effect}}{\underbrace{X_i \beta}} + \underset{\text{random effect}}{\underbrace{Z_i b_i}} + \varepsilon_i$$`

Where:

$$
\begin{cases}
    b_i \sim N(0, D) \\
    \varepsilon_i \sim N(0, \Sigma_i) \\
    b_1, b_2, \ldots, b_N, \varepsilon_1, \varepsilon_2, \ldots, \varepsilon_N \,  are \, , independent
\end{cases}
$$

**Terminology:**

- Fixed effects: `\(\beta\)`
- Random effects: `\(b_i\)`
- Variance components: `\(D\)` and `\(\Sigma_i\)`

---

- For Gaussian data, &lt;span style="color: blue; font-weight: bold;"&gt;GLMM&lt;/span&gt; extends the General Linear Model (GLM) by the addition of random effect parameters and by allowing a more flexible specification of the covariance matrix of the random errors.

- &lt;span style="color: blue; font-weight: bold;"&gt;GLM&lt;/span&gt;: `\(Y_i = X_i\beta + \epsilon_{i}\)`
  - &lt;span style="color: blue; font-weight: bold;"&gt;GLM&lt;/span&gt; includes t-tests, analysis of variance (ANOVA), correlation, regression, and factor analysis, etc.

- &lt;span style="color: blue; font-weight: bold;"&gt;GLMM&lt;/span&gt;: `\(Y_i = X_i\beta + Z_ib_i+ \epsilon_{i}\)`

- Difference?

- `\(\epsilon_{i}\)`
  - &lt;span style="color: blue; font-weight: bold;"&gt;GLM&lt;/span&gt;: vector of random errors
  - &lt;span style="color: blue; font-weight: bold;"&gt;GLMM&lt;/span&gt;: is no longer required to be independent and homogenous

- &lt;span style="color: blue; font-weight: bold;"&gt;Mixed Effects Models&lt;/span&gt;
  - Applicable to all types of outcomes (continuous, discrete)
  - Can handle both time-variant and time-invariant covariates
  - Robust to missing data (irregularly spaced observations)

---

- Contains both fixed and random effects

`\(Y_i = X_i\beta+ Z_ib_i + \epsilon_i\)`

&lt;img src="Image/fig12.png" width="80%" style="display: block; margin: auto;" /&gt;


---

#### Summary

- LMM extends the GLM by the addition of random effect parameters and by allowing a more flexible specification of the covariance matrix of the random errors

- LMM can easily fitted to longitudinal data

- Estimation is more difficult in mixed models than GLM

- Longitudinal models have three sources of variation

 - between subject variability/represented by random effect
 
 - Within subject variability/represented by serial correlation
 
 - Measurement error


---

#### Source of random variation

- `\(Y_i = X_i\beta + Z_i b_i + \epsilon_i\)`
- `\(Y_i = X_i\beta + Z_i b_i + \epsilon_{(1)i} + \epsilon_{(2)i}\)`

**3 stochastic components:**

- `\(b_i\)`: &lt;span style="color: blue;"&gt;between-participant variability&lt;/span&gt;
- `\(\epsilon_{(1)i}\)`: &lt;span style="color: blue;"&gt;measurement error&lt;/span&gt;
- `\(\epsilon_{(2)i}\)`: &lt;span style="color: blue;"&gt;serial correlation component&lt;/span&gt;

**Random effects (variation between participants)**

- &lt;span style="color: blue;"&gt;Characteristics of individual participants&lt;/span&gt;
- For example, &lt;span style="color: blue;"&gt;intrinsically high or low responders&lt;/span&gt;

**Serial correlation (variation over time within participants)**

- Measurements taken close together in time are &lt;span style="color: blue;"&gt;strongly correlated&lt;/span&gt; than those taken further apart in time
- On a sufficiently small scale, this kind of structure is &lt;span style="color: blue;"&gt;almost inevitable&lt;/span&gt;

**Measurement error**

- &lt;span style="color: blue;"&gt;Extra component of measurement error&lt;/span&gt; reflecting variation added due to the measurement process.

---

#### Model Families

- &lt;span style="color: blue;"&gt;Marginal (population average) models&lt;/span&gt;

- &lt;span style="color: blue;"&gt;Subject specific models&lt;/span&gt;

- &lt;span style="color: blue;"&gt;Conditional models&lt;/span&gt;

Each can be 

- &lt;span style="color: blue;"&gt;Random intercept model&lt;/span&gt;

- &lt;span style="color: blue;"&gt;Random slope model&lt;/span&gt;

- &lt;span style="color: blue;"&gt;Random higher order model&lt;/span&gt;


---

#### Model families

&lt;span style="color: blue;"&gt;Marginal (population average) models&lt;/span&gt;

- Responses are marginalized over all other responses
- Parameters characterize the marginal expectation

&lt;span style="color: blue;"&gt;Subject specific models&lt;/span&gt;

- If the aim is to study how subjects change overtime &amp; what characteristics influence such changes
- Subject specific models differ from marginal models by the inclusion of parameters specific to the subject

&lt;span style="color: blue;"&gt;Conditional models&lt;/span&gt;

- Any response within the sequence of repeated measures is modeled conditional upon the other outcomes
- Parameters describe a feature of outcomes, given values for the other outcomes (Cox 1972) i.e. log linear models



---

#### 1. Random intercept model 

`$$\begin{cases}
    Y_{ij} = \beta_0 + \beta_1t_{ij} + \color{blue}{b_{0i}} + \epsilon_{ij}\\
    b_{0i} \sim N(0,\sigma_b^2),  
    \varepsilon_i \sim N(0, \sigma_{\epsilon}^2) \\
    b_{0i}, \varepsilon_{ij} \,  are \, \, independent
\end{cases}$$`
- Each subject has his/her own intercept: `\(\color{blue}{\beta_0 + b_{0i}}\)`

- Interparticipant variability at **baseline**

- Slope remains the same: `\(\color{blue}{\beta_1}\)`

- Fixed effects can be added to the model

**Conditional distribution:**

- Take `\(\color{blue}{E(Y/b)}\)` and `\(\color{blue}{Var(Y/b)}\)`

 - `\(\color{red}{E(Y_{ij}/b_{0i}) = \beta_0 + \beta_1t_{ij} + b_{0i}}\)`
 
 - `\(\color{red}{Var(Y_{ij}/b_{0i}) = \sigma^2_{\epsilon}}\)`
 
 - `\(\color{red}{Y_{ij}/b_{0i}\sim N(\beta_0 + \beta_1t_{ij} + b_{0i}, \sigma^2_{\epsilon})}\)`

---

#### 1. Random intercept model 

`$$\begin{cases}
    Y_{ij} = \beta_0 + \beta_1t_{ij} + \color{blue}{b_{0i}} + \epsilon_{ij}\\
    b_{0i} \sim N(0,\sigma_b^2),  
    \varepsilon_i \sim N(0, \sigma_{\epsilon}^2) \\
    b_{0i}, \varepsilon_{ij} \,  are \, \, independent
\end{cases}$$`
**Marginal distribution (marginal over the random intercepts):**

- Take `\(\color{blue}{E(Y_{ij})}\)` and `\(\color{blue}{Var(Y_{ij})}\)`

 - `\(\color{red}{E(Y_{ij}) = \beta_0 + \beta_1t_{ij}}\)`
 
 - `\(\color{red}{Var(Y_{ij}) = \sigma_{b}^2 + \sigma_{\epsilon}^2}\)`
 
 - `\(\color{red}{Y_{ij}\sim N(\beta_0 +\beta_1t_{ij}, \sigma_b^2 + \sigma_{\epsilon}^2)}\)`


---

#### The random intercept model: ICC

- Measurements from the same participant share a random effect:
  - Means that marginally, there is a &lt;span style="color: blue;"&gt;correlation structure&lt;/span&gt;
  

- Let's consider two measurements from the same participant:
  
  - `\(Y_{ij}\)` and `\(Y_{ik}\)`, `\(j\neq k\)`
  
  - `\(Cov(Y_{ij},Y_{ik}) = \sigma_b^2\)`


- &lt;span style="color: blue;"&gt;Correlation between two measurements from the same participant:&lt;/span&gt;

`$$Cov(Y_{ij},Y_{ik}) = \frac{\sigma_b^2}{\sigma_b^2 + \sigma_{\epsilon}^2}$$`

---

#### 2. Random Slope Model

.pull-left[
`$$\begin{cases}
    Y_{ij} = \beta_0 + \beta_1t_{ij} + \color{blue}{b_{0i}+ b_{1i}t_{ij}} + \epsilon_{ij}\\
    b_{0i}, b_{1i} \sim N(0, D),  
    \varepsilon_i \sim N(0, \sigma_{\epsilon}^2) \\
    \epsilon_{ij} \, independent \ of \ b_{0i}, b_{1i}
\end{cases}$$`
]
.pull-right[
`$$D = \begin{pmatrix} \sigma_{0}^2 &amp; \sigma_{01} \\\\ \sigma_{10} &amp; \sigma_{1}^2 \end{pmatrix}$$`
]

- Each subject has his/her own intercept: `\(\color{blue}{\beta_0 + b_{0i}}\)`
- Each subject has his/her own slope: `\(\color{blue}{\beta_1 + b_{1i}}\)`
- Allows the profiles to cross each other
- Fixed effects can be added to the model.

- This model has two random effects: `\(\color{blue}{b_{0i}}\)` and `\(\color{blue}{b_{1i}}\)`

- Their covariance `\(\color{blue}{\sigma_{10} = \sigma_{01}:}\)`

  - If positive: subjects higher at baseline also have a &lt;span style="color: blue;"&gt;higher evolution&lt;/span&gt;
  - If negative: subjects higher at baseline have a &lt;span style="color: blue;"&gt;slower evolution&lt;/span&gt;

---

#### 2. Random Slope Model

- If the covariance `\(\color{blue}{\sigma_{10} = \sigma_{01}}\)` is not restricted:
  - It is called &lt;span style="color: red;"&gt;unstructured&lt;/span&gt;.
  
**Conditional distribution:**

- `\(E(Y_{ij} | b_{0i}, b_{1i}) = \beta_0 + \beta_1t_{ij} + b_{0i} + b_{1i}t_{ij}\)`
- `\(Var(Y_{ij} | b_{0i}, b_{1i}) = \sigma_{\epsilon}^2\)`

**Marginal distribution:**

- `\(E(Y_{ij}) = \beta_0 + \beta_1t_{ij}\)`
- `\(Var(Y_{ij}) = \sigma_1^2 t_{ij} + 2\sigma_{01} t_{ij} + \sigma_{0}^2 + \sigma_{\epsilon}^2\)`

- Note: &lt;span style="color: blue;"&gt;marginal variance is a function of time&lt;/span&gt;

---

#### The Random Slope Model: ICC

- Let's consider two measurements from the same subject:
  - `\(Y_{ij}\)` and `\(Y_{ik}\)`, `\(j \neq k\)`
  - `\(Cov(Y_{ij}, Y_{ik}) = \sigma_1^2 t_{ij} t_{ik} + \sigma_{01}(t_{ij} + t_{ik}) + \sigma_0^2\)`

- The ICC is now a function of time:
  `$$Corr(Y_{ij}, Y_{ik}) = 
  \frac{\sigma_1^2 t_{ij} t_{ik} + \sigma_{01}(t_{ij} + t_{ik}) + \sigma_0^2}{\sqrt{\sigma_1^2 t_{ij} + 2\sigma_{01} t_{ij} + \sigma_0^2 + \sigma_{\epsilon}^2} \sqrt{\sigma_1^2 t_{ik} + 2\sigma_{01} t_{ik} + \sigma_0^2 + \sigma_{\epsilon}^2}}$$`

---

#### Marginal models Vs Subject specific Models

3. The General Linear Mixed-effects model:

`$$\color{red}{Y_i = X_i\beta + Z_ib_i + \epsilon_i}$$`

`$$b_i \sim N(0, D), \epsilon_i \sim N(0, \Sigma_i)$$`; `\(b_1, b_2, \ldots, b_N, \epsilon_1, \epsilon_2, \ldots, \epsilon_N\)` independent

- It can be written as:

`$$Y_i/b_i \sim N(\color{red}{X_i\beta + Z_ib_i}, \Sigma_i)$$`

`$$b_i \sim N(0, D)$$`

---
#### Marginal models Vs Subject specific Models

It is also called a &lt;span style="color: blue;"&gt;hierarchical model&lt;/span&gt;:
- A model for `\(Y_i\)` given `\(b_i\)`
- A model for `\(b_i\)`

Marginally, we have that `\(Y_i\)` distributed as:
`$$Y_i \sim N(X_i\beta, \color{red}{Z_iDZ_i' + \Sigma_i})$$`

Hence, very specific assumptions are made about the dependence of mean and covariance on the covariates `\(X_i\)` and `\(Z_i\)`:

 - Implied mean: `\(\color{red}{X_i\beta}\)`

 - Implied covariance: `\(\color{red}{V_i = Z_iDZ_i' + \Sigma_i}\)`

&lt;span style="color: blue;"&gt;Note that the hierarchical model implies the marginal, NOT vice versa.&lt;/span&gt;

---

#### Marginal models vs subject specific models ... (3)


&lt;img src="Image/fig13.png" width="70%" style="display: block; margin: auto;" /&gt;

---
class: inverse, middle

#### Marginal model: estimation and inference

- Estimation of the marginal model
 - Introduction
 - Maximum likelihood estimation (MLE)
 - Restricted maximum likelihood estimation (RMLE)
 
- General guidelines for model building

- Estimation and tests for random effects

- Practical: Fitting LMM in R

---

#### Recap: The General Linear Mixed effects Model (GLMM)

- GLMM:

`\(Y_i = X_i\beta + Z_ib_i+ \epsilon_{i}\)`

where
 - `\(b_i \sim N(0,D)\)`
 - `\(\epsilon_{i}\sim N(0,\Sigma_i)\)`
 - `\(b_1, b_2, ..., b_N, \epsilon_{1}, \epsilon_{2}, ..., \epsilon_{N}\)` independent

- Terminology:

 - Fixed effects: `\(\beta\)`
 
 - Random effects: `\(b_i\)`
 
 - Variance components: `\(𝐷\)` and `\(\Sigma_i\)`


---

#### Estimation of the marginal model

- Recall: GLMM

`\(Y_i = X_i\beta + Z_ib_i+ \epsilon_{i}\)`

where
 `\(b_i \sim N(0,D)\)`, `\(\epsilon_{i}\sim N(0,\Sigma_i)\)`
 
 `\(b_1, b_2, ..., b_N, \epsilon_{1}, \epsilon_{2}, ..., \epsilon_{N}\)` independent

- The implied marginal model equals: `\(Y_{i}\sim N(X_i\beta, Z_iDZ_{i}'+ \Sigma_i)\)`
- Let:
 - Residual error covariance matrix: `\(\Sigma_i = \sigma_i^2I_{n_i}\)`
 - The marginal covariance matrix: `\(V_i = Z_iDZ_{i}'+ \Sigma_i\)`
- Inferences based on the marginal model do not explicitly assume the presence of random effects representing the natural heterogeneity between subjects

---

**Notation:**

 - `\(\beta\)`: vector of fixed effects (as before)
 - `\(\alpha\)`: Vector of all variance components in `\(D\)` and `\(\Sigma_i\)`
 - `\(\theta = (\beta',\alpha')\)` vector of all parameters in marginal model

- Marginal likelihood function is given by:

`$$L_{LM}(\theta) = \prod_{i=1}^N \left\{ (2\pi)^{-n_i/2} |V_i(\alpha)|^{-1/2} \exp\left(-\frac{1}{2} (Y_i-X\beta)^T V_i(\alpha)^{-1} (Y_i-X\beta) \right) \right\}$$`
If `\(\alpha\)` were known, the maximum likelihood estimate (MLE) of `\(\beta\)` would be: 
`$$\hat{\beta}(\alpha) = \left( \sum_{i=1}^N (X_i^T W_i X_i)^{-1} \sum_{i=1}^N X_i^T W_i y_i \right)$$`

where `\(W_i = V^{-1}\)`.


---

- In most cases, `\(\alpha\)` were unknown, and needs to be replaced by
an estimate `\(\hat{\alpha}\)`

- Two frequently used estimation methods for `\(\alpha\)` 
  - Maximum likelihood (ML)
  - Restricted maximum likelihood (REML)
  
---

#### Maximum Likelihood Estimation (MLE)

- **ML estimation of `\(V_i\)`**:
  - Does not take into account that `\(\beta\)` estimated from data.
  
  - Does not account for degrees of freedom lost.
  
  - Generally results in biased estimation of `\(V_i\)`.

---

#### Restricted Maximum Likelihood Estimation (REML)

- **What’s the difference between ML and REML?**
  - ML estimates of variances are known to be biased in small samples.
  - The simplest case: Sample variance
  
 `$$Var(x) = \frac{1}{n - 1} \sum_{i=1}^{n} (x_i - \bar{x})^2$$`
 
  - To obtain an unbiased estimate, we need to divide by `\(n-1\)` because we estimate the mean
    
      `$$\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$`
- The REML estimation is a generalization of this idea.

- It provides unbiased estimates of the parameters in the covariance matrix `\(V_i\)` in small samples.

---

#### Features of REML estimation:

- It corrects for the downward bias in the ML parameters.

- It handles strong correlations among the responses more effectively.

- Available in all software that fit marginal and mixed effects models.

- The default estimation method in most software (R, SAS).

- It works is by applying a transformation in the longitudinal outcome 𝑌 based on the chosen structure of the design matrix 𝑋(i.e., which predictors you have included in the model).

- Models with different mean structures not comparable.

 - Since different observations involved.

---

#### Features of REML estimation

- It corrects for the downward bias in the ML parameters

- It handles strong correlations among the responses more effectively

- Available in all software that fit marginal and mixed effects models

- The default estimation method in most software (R, SAS)

- It works by applying a transformation in the longitudinal outcome `\(𝑌\)` based on the chosen structure of the design matrix `\(𝑋\)` (i.e., which predictors you have included in the model)

- Models with different mean structures not comparable.
 - since different observations involved.

---

#### ML versus REML

- Both are based on the likelihood principle, which has the properties of consistency, asymptotic normality, and efficiency.

- The differences between ML and REML estimation increase as the number of fixed effects in the model increases.

- Difference between ML and REML is less marked if `\(n&gt;p\)`

- We cannot compare the likelihoods of models fitted with REML and have different `\(X\beta\)` part!


---

#### Components of the linear mixed Effects model (LMM)

- `\(Y_i = X_i\beta + Z_ib_i+ \epsilon_{i}\)`


- The implied marginal model equals: `\(Y_{i}\sim N(X_i\beta, Z_iDZ_{i}'+ \Sigma_i)\)`
- The mean structure: `\(X_i\beta\)`

- The covariance structure:

 - When we estimate the covariance matrix without making any particular assumption about the covariance structure, we say that we are using an unrestricted or unstructured covariance matrix (UN).
 
 - As we shall see later, it is sometimes advantageous to model
the covariance structure more parsimoniously.

---

#### Fitting marginal models in R

- **R&gt;** The following code fits a marginal model for the growth data with a compound symmetry correlation structure:


```r
gls.Symm &lt;- gls(distance ~ Sex * I(age - 11), data = growth, 
                correlation = corSymm(form = ~1|Subject),
                weights = varIdent(form = ~1|age))
```

- The Restricted Maximum Likelihood (REML) method is commonly used as the default estimation approach for the Generalized Least Squares (GLS).

---

#### Covariance matrix ... (1)

Variances, covariances and correlations

- variance measures how far a set of numbers is spread out (always positive)

- covariance is a measure of how much two random variables change together (positive or negative)

- correlation a measure of the linear correlation (dependence) between two variables (between −1 and 1; 0 no correlation)

- We need an appropriate choice for the marginal covariance matrix: `\(V_i = Z_iDZ_{i}'+ \Sigma_i\)` in order to appropriately describe the correlations between the repeated measurements

- Mostly used appropriate for the research question under study– independent, compound symmetry or exchangeable, autoregressive, unstructured.

---
#### Covariance matrix ... (2)

&lt;img src="Image/workcorr.png" width="60%" style="display: block; margin: auto;" /&gt;


- Independent covariance structure
 - assumes that the measurements are&lt;span style="color: red;"&gt; uncorrelated.&lt;/span&gt; i.e., no linear relationship between them.
- Compound symmetry covariance structure
 - assumes that all correlations between measurements are &lt;span style="color: red;"&gt;equal.&lt;/span&gt;  
- Autoregressive covariance structure
 - assumes that &lt;span style="color: red;"&gt;correlations decrease as the time interval between measurements increases.&lt;/span&gt;   
    - As the time interval increases, the correlation decreases exponentially.
- Unstructured covariance structure
 - &lt;span style="color: red;"&gt;makes no assumptions about the correlations between measurements.&lt;/span&gt;  

---

#### Example code


```r
library(nlme)
# Independent covariance structure
fit_i &lt;- lme(response ~ time, random = ~ 1 | subject, 
             correlation = corSymm(form = ~ 1))

# Compound symmetry covariance structure
fit_c &lt;- lme(response ~ time, random = ~ 1 | subject, 
             correlation = corCompSymm(form = ~ 1))

# Autoregressive covariance structure
fit_ar &lt;- lme(response ~ time, random = ~ 1 | subject, 
              correlation = corAR1(form = ~ time))

# Unstructured covariance structure
fit_un &lt;- lme(response ~ time, random = ~ 1 | subject, 
              correlation = corSymm(form = ~ 1))
```

---

#### Model building

 - We have seen that marginal models consist of two parts:

   - Mean part `\(X\beta:\)` that describes how covariates we have put in the model explain the average of the repeated measurements.
   - Covariance part `\(V_i:\)` assumed covariance structure between the repeated measurements.

- In the majority of the cases, scientific interest focuses on the mean part.

- However, to obtain valid and efficient inferences for the mean part, the covariance part needs to be adequately specified.


---

#### General guidelines for model building

- Exploratory data analysis Descriptive statistics, individual group
profiles, plots

- Begin with simple models and build towards more complex mean structure
 - Put all the covariates of interest in the mean part, considering possible nonlinear and interaction terms - do NOT remove the ones that are not significant

- Then select covariance structure covariance matrix `\(V_i\)` that adequately describes the correlations in the repeated measurements

- Finally, reduce the mean structure

 - return to the mean part and exclude non significant covariates
 - start by testing the interaction terms, and then the nonlinear terms

- Model diagnostics


---

#### Fitting linear mixed models in R ... further details

- There are two packages in R for fitting multilevel models

- The older and more comprehensive package is `nlme`, an acronym for &lt;span style="color: blue;"&gt;nonlinear mixed effects models&lt;/span&gt;

- Its limitation is that it only fits &lt;span style="color: blue;"&gt;normal-based models&lt;/span&gt; and was not designed to fit mixed models to &lt;span style="color: blue;"&gt;non-hierarchical data&lt;/span&gt;

- The newer package is `lme4`

- It can handle &lt;span style="color: blue;"&gt;generalized linear mixed effect regression models&lt;/span&gt; such as logistic and Poisson regression

- It currently lacks the &lt;span style="color: blue;"&gt;nonlinear features&lt;/span&gt; of `nlme`

- Since we are going to focus on examples based on normal theory, our focus will be on the `nlme` package

---

#### Model: Jimma infant data

`$$W_{ij} = \beta_0 + b_{0i} + \beta_1S_i + (\beta_2 + b_{1i})A_{ij} + \beta_3{A_{ij}}^2 + \beta_4S_iA_{ij} + \beta_5{A_{ij}}^2S_i + \varepsilon_{ij}$$`

- `\(W_{ij}\)`: weight (Kg) of the `\(i^{th}\)` infant at the `\(j^{th}\)` visit.

- `\(A_{ij}\)`: Age of the `\(i^{th}\)` infant at the `\(j^{th}\)` visit.

- `\(S_i\)`: Sex of the `\(i^{th}\)` infant (&lt;span style="color: blue;"&gt;Female=0, Male=1&lt;/span&gt;)

- `\(b_{0i}\)`: &lt;span style="color: blue;"&gt;random intercept&lt;/span&gt;; `\(b_{1i}\)`: &lt;span style="color: blue;"&gt;random slope&lt;/span&gt;

---

#### Basic components from R

- The function &lt;span style="color: green;"&gt;`lme`&lt;/span&gt; under the library &lt;span style="color: green;"&gt;`nlme`&lt;/span&gt; in R fits:
  - &lt;span style="color: blue;"&gt;Linear mixed-effects model&lt;/span&gt;
  - &lt;span style="color: blue;"&gt;Multilevel linear mixed effects model&lt;/span&gt;

- It uses &lt;span style="color: blue;"&gt;maximum likelihood&lt;/span&gt; or &lt;span style="color: blue;"&gt;restricted maximum likelihood&lt;/span&gt;

- The command &lt;span style="color: green;"&gt;`lme`&lt;/span&gt; in R is as follows:


```r
lme(fixed, data, random, correlation, weights, subset, 
    method, na.action, control, contrasts = NULL, keep.data = TRUE)
```

- &lt;span style="color: blue;"&gt;`fixed`&lt;/span&gt; is an argument to define the fixed effects portion.
- &lt;span style="color: blue;"&gt;`random`&lt;/span&gt; is an argument to define the random effects portion.
- &lt;span style="color: blue;"&gt;`data`&lt;/span&gt; is an optional data frame containing the variables named &lt;span style="color: blue;"&gt;`correlation`&lt;/span&gt; describing the within-group correlation structure.
- &lt;span style="color: blue;"&gt;`method`&lt;/span&gt; is an argument to &lt;span style="color: blue;"&gt;`lme`&lt;/span&gt; that changes the estimation method.

---

- &lt;span style="color: blue;"&gt;REML&lt;/span&gt;: the model is fit by maximizing the restricted log-likelihood.
- If &lt;span style="color: blue;"&gt;ML&lt;/span&gt;, the log-likelihood is maximized. The Default is REML.

**Fixed and Random Parts:**
   - The &lt;span style="color: blue;"&gt;fixed part&lt;/span&gt; is `fixed = distance ∼ Sex + Sex ∗ age`.
   - The &lt;span style="color: blue;"&gt;random part&lt;/span&gt; is `random =∼ 1|Subject`.

- If the random part is specified as above, it means we will fit a model with a &lt;span style="color: blue;"&gt;random intercept&lt;/span&gt;.

- Here, the response is specified only on the fixed part.

- In the random part, the model statement begins with just a ∼.

- If the random formula is omitted, its default value is taken as the right-hand side of the fixed formula.

- The vertical bar separates the model specification from the structural specification.

---
**Model:**
`$$D_{ij} = \beta_0 + \beta_1 S_i + \beta_2 A_{0ij} + \beta_4 S_i A_{0ij} + b_{0i} + b_{1i} A_{0ij} + \varepsilon_{ij}$$`

- `\(D_{ij}\)`: Orthodontic distance of the `\(i^{th}\)`h child at the j^{th}$ visit.
- `\(A_{ij}\)`: Age of the `\(i^{th}\)` child at the `\(j^{th}\)` visit, `\(A_{0ij} = A_{ij} - 8\)`
- `\(S_i\)`: Sex of the `\(i^{th}\)` child (boys = 1, girls = 2)
- `\(b_{0i}\)`: Random intercept
- `\(b_{1i}\)`: Random slope

---
**Growth Data**

We want to fit a random intercept model on growth data, and the following code can be used:


```r
library(nlme)
growth$age &lt;- as.numeric(growth$age)
growth.fit1 &lt;- lme(fixed = measure ~ Sex + Sex * age,
                   data = growth, random = ~ 1 | ind)
```
**Code Explanation:**

- Fixed effect: `fixed = distance ~ Sex + Sex * age`
- Name for the data: `data = growth`
- Random intercept: `random = 1 | ind`

For the `growth.fit1` object, `print(growth.fit1)` gives...

---


```r
print(growth.fit1)
```

```
## Linear mixed-effects model fit by REML
##   Data: growth 
##   Log-restricted-likelihood: -201.6885
##   Fixed: measure ~ Sex + Sex * age 
##   (Intercept)     Sexfemale           age Sexfemale:age 
##    21.0213732    -0.8683341     1.5738504    -0.5953673 
## 
## Random effects:
##  Formula: ~1 | ind
##         (Intercept) Residual
## StdDev:    1.836499 1.440418
## 
## Number of Observations: 99
## Number of Groups: 27
```

---

**Main &lt;span style="color: blue;"&gt;`lme`&lt;/span&gt; Methods:**

- &lt;span style="color: blue;"&gt;`ACF`&lt;/span&gt;: Empirical autocorrelation function of within-group residuals
- &lt;span style="color: blue;"&gt;`anova`&lt;/span&gt;: Likelihood ratio or conditional tests
- &lt;span style="color: blue;"&gt;`augPred`&lt;/span&gt;: Predictions augmented with observed values
- &lt;span style="color: blue;"&gt;`coef`&lt;/span&gt;: Estimated coefficients for different levels of grouping
- &lt;span style="color: blue;"&gt;`fitted`&lt;/span&gt;: Fitted values for different levels of grouping
- &lt;span style="color: blue;"&gt;`fixef`&lt;/span&gt;: Fixed-effects estimates
- &lt;span style="color: blue;"&gt;`intervals`&lt;/span&gt;: Confidence intervals on model parameters
- &lt;span style="color: blue;"&gt;`logLik`&lt;/span&gt;: Log-likelihood at convergence
- &lt;span style="color: blue;"&gt;`pairs`&lt;/span&gt;: Scatter-plot matrix of coefficients or random effects
- &lt;span style="color: blue;"&gt;`plot`&lt;/span&gt;: Diagnostic Trellis plots
- &lt;span style="color: blue;"&gt;`predict`&lt;/span&gt;: Predictions for different levels of grouping
- &lt;span style="color: blue;"&gt;`print`&lt;/span&gt;: Brief information about the fit
- &lt;span style="color: blue;"&gt;`qqnorm`&lt;/span&gt;: Normal probability plots
- &lt;span style="color: blue;"&gt;`ranef`&lt;/span&gt;: Random-effects estimates
- &lt;span style="color: blue;"&gt;`resid`&lt;/span&gt;: Residuals for different levels of grouping
- &lt;span style="color: blue;"&gt;`summary`&lt;/span&gt;: More detailed information about the fit
- &lt;span style="color: blue;"&gt;`update`&lt;/span&gt;: Update the &lt;span style="color: blue;"&gt;`lme`&lt;/span&gt; fit
- &lt;span style="color: blue;"&gt;`Variogram`&lt;/span&gt;: Semivariogram of within-group residuals

---
The command `coef(growth.fit1)` in R produced;


```r
coef(growth.fit1)
```

```
##    (Intercept)  Sexfemale     age Sexfemale:age
## 1     19.96031 -0.8683341 1.57385    -0.5953673
## 2     21.36871 -0.8683341 1.57385    -0.5953673
## 3     21.77183 -0.8683341 1.57385    -0.5953673
## 4     22.99378 -0.8683341 1.57385    -0.5953673
## 5     21.04369 -0.8683341 1.57385    -0.5953673
## 6     19.69724 -0.8683341 1.57385    -0.5953673
## 7     21.36871 -0.8683341 1.57385    -0.5953673
## 8     21.69372 -0.8683341 1.57385    -0.5953673
## 9     19.69724 -0.8683341 1.57385    -0.5953673
## 10    17.34603 -0.8683341 1.57385    -0.5953673
## 11    24.29384 -0.8683341 1.57385    -0.5953673
## 12    23.44295 -0.8683341 1.57385    -0.5953673
## 13    19.73377 -0.8683341 1.57385    -0.5953673
## 14    20.40948 -0.8683341 1.57385    -0.5953673
## 15    22.46791 -0.8683341 1.57385    -0.5953673
## 16    19.04224 -0.8683341 1.57385    -0.5953673
## 17    22.25123 -0.8683341 1.57385    -0.5953673
## 18    19.97613 -0.8683341 1.57385    -0.5953673
## 19    20.08446 -0.8683341 1.57385    -0.5953673
## 20    21.16785 -0.8683341 1.57385    -0.5953673
## 21    24.95969 -0.8683341 1.57385    -0.5953673
## 22    19.86779 -0.8683341 1.57385    -0.5953673
## 23    20.42530 -0.8683341 1.57385    -0.5953673
## 24    20.14868 -0.8683341 1.57385    -0.5953673
## 25    20.95117 -0.8683341 1.57385    -0.5953673
## 26    21.81788 -0.8683341 1.57385    -0.5953673
## 27    19.59546 -0.8683341 1.57385    -0.5953673
```
---
Command fixef (Ortho.fit1), the following output is produced

```r
fixef(growth.fit1)
```

```
##   (Intercept)     Sexfemale           age Sexfemale:age 
##    21.0213732    -0.8683341     1.5738504    -0.5953673
```


- The parameters are average

---

- Producing Maximum Likelihood Estimates Using &lt;span style="color: blue;"&gt;lme&lt;/span&gt;

- In all of the above outputs, we produced the Restricted Maximum

- Likelihood Estimates as REML is the default method in the &lt;span style="color: blue;"&gt;lme&lt;/span&gt;.

- The argument &lt;span style="color: blue;"&gt;method=ML&lt;/span&gt; requests that estimates be obtained using full maximum likelihood.


```r
growth.fit2 &lt;-lme(fixed = measure ~ Sex+Sex*age, method= "ML",
data = growth, random = ~ 1|ind)
```


- The output that follows is based on the maximum likelihood
estimation.

- The &lt;span style="color: blue;"&gt;intervals growth.fit2&lt;/span&gt; command in R will produce the following confidence interval for the parameters of our model.

---

```r
intervals(growth.fit2)
```

```
## Approximate 95% confidence intervals
## 
##  Fixed effects:
##                   lower       est.       upper
## (Intercept)   19.768981 21.0233697 22.27775872
## Sexfemale     -2.900448 -0.8676887  1.16507014
## age            1.253491  1.5734511  1.89341086
## Sexfemale:age -1.097287 -0.5954963 -0.09370524
## 
##  Random Effects:
##   Level: ind 
##                    lower     est.    upper
## sd((Intercept)) 1.281518 1.759342 2.415326
## 
##  Within-group standard error:
##    lower     est.    upper 
## 1.206171 1.420339 1.672535
```

---

- The summary(growth.fit2) command in R will produce the following output for the
parameters of our model


```r
summary(growth.fit2)
```

```
## Linear mixed-effects model fit by maximum likelihood
##   Data: growth 
##        AIC      BIC    logLik
##   413.3128 428.8835 -200.6564
## 
## Random effects:
##  Formula: ~1 | ind
##         (Intercept) Residual
## StdDev:    1.759342 1.420339
## 
## Fixed effects:  measure ~ Sex + Sex * age 
##                   Value Std.Error DF  t-value p-value
## (Intercept)   21.023370 0.6420483 70 32.74422  0.0000
## Sexfemale     -0.867689 1.0075619 25 -0.86118  0.3973
## age            1.573451 0.1637687 70  9.60777  0.0000
## Sexfemale:age -0.595496 0.2568375 70 -2.31857  0.0233
##  Correlation: 
##               (Intr) Sexfml age   
## Sexfemale     -0.637              
## age           -0.651  0.415       
## Sexfemale:age  0.415 -0.651 -0.638
## 
## Standardized Within-Group Residuals:
##          Min           Q1          Med           Q3          Max 
## -3.331674862 -0.531945925 -0.009607243  0.482544286  3.599008895 
## 
## Number of Observations: 99
## Number of Groups: 27
```

---

- The &lt;span style="color: blue;"&gt;maximum likelihood&lt;/span&gt; is the estimation method that was used.

- The &lt;span style="color: blue;"&gt;AIC&lt;/span&gt; and &lt;span style="color: blue;"&gt;log likelihood&lt;/span&gt; can be used to make comparisons between models with different fixed effects (or random effects).

- The next estimates for the &lt;span style="color: blue;"&gt;random effects&lt;/span&gt; part of the model.

- In the line where the numerical estimates appear, the label is &lt;span style="color: blue;"&gt;StdDev&lt;/span&gt;, indicating that standard deviations are displayed.

- The estimates displayed are the standard deviations of &lt;span style="color: blue;"&gt;between variability&lt;/span&gt; `\((\sigma_b = 1.74)\)` and the standard deviations of &lt;span style="color: blue;"&gt;within variability&lt;/span&gt; `\((\sigma_w = 1.369)\)`.

- In the &lt;span style="color: blue;"&gt;Fixed Effects&lt;/span&gt; sections, we have the reported value of the intercept, its estimated standard error, and &lt;span style="color: blue;"&gt;Wald test&lt;/span&gt; for whether its value is significantly different from zero or not.

---

**Random Slope Model:**

- Random intercept: `random = 1|ind`
- Random intercept and slope: `random =∼ age|subject`


```r
growth.fit2 &lt;- lme(fixed = measure ~ Sex + Sex * age,
                   data = growth, random = ~ age | ind)
```

*VarCorr(growth.fit2)*, variance components can be extracted from the
model


```r
VarCorr(growth.fit2)
```

```
## ind = pdLogChol(age) 
##             Variance  StdDev    Corr  
## (Intercept) 4.3613844 2.0883928 (Intr)
## age         0.1766018 0.4202402 -0.457
## Residual    1.7665539 1.3291177
```

---

**Inference for the Marginal Model**

- Having fitted a marginal model using maximum likelihood, we can use standard inferential tools for performing hypothesis testing:
  - &lt;span style="color: blue;"&gt;Wald test&lt;/span&gt;
  - &lt;span style="color: blue;"&gt;t-test / F-test&lt;/span&gt;
  - Score test
  - Likelihood ratio test (LRT)
  - Robust Inference
  
- Following the model building strategy described above, we will:
  - First, describe how we can choose the appropriate covariance matrix, and
  - Then focus on hypothesis testing for the mean part of the model.

---
**Hypothesis Testing for `\(V_i\)`:**

- Assuming the same mean structure, we can fit a series of models and choose the one that best describes the covariances.

- In general, we distinguish between two cases:
  - Comparing two models with &lt;span style="color: blue;"&gt;nested covariance matrices&lt;/span&gt;
  - Comparing two models with &lt;span style="color: blue;"&gt;non-nested covariance matrices&lt;/span&gt;

- &lt;span style="color: blue;"&gt;Model A is nested in Model B&lt;/span&gt; when Model A is a special case of Model B – i.e., by setting some of the parameters of Model B at some specific value, we obtain Model A.

---
**For nested models, the preferable test for selecting `\(V_i\)` is the &lt;span style="color:purple; font-weight:bold;"&gt;likelihood ratio test (LRT)&lt;/span&gt;:**

The &lt;span style="color:purple; font-weight:bold;"&gt;likelihood ratio test (LRT)&lt;/span&gt; is calculated as follows:

`$$LRT = -2 \left( \ell(\theta_0) - \ell(\theta_a) \right)$$`

Where:

- `\(\ell(\theta_0)\)` is the value of the log-likelihood function under the null hypothesis, i.e., the special case model.
- `\(\ell(\theta_a)\)` is the value of the log-likelihood function under the alternative hypothesis, i.e., the general model.
- `\(p\)` denotes the number of parameters being tested.

**Note:** Provided that the mean structure in the two models is the same, we can either compare the &lt;span style="color:blue;"&gt;REML or ML likelihoods&lt;/span&gt; of the models (preferably REML).


---

- We can rewrite the two hypotheses as:


  `$$LRT = -2 \left( \ell(\theta_0) - \ell(\theta_a) \right) \sim \chi_p^2$$`

`$$H_0:
\begin{cases}
    σ_{12} = σ_{22} = σ_{32} = σ_{42} = σ^2 \\
    σ_{12} = σ_{13}, = \dots = σ_{34} = \tilde{σ}
\end{cases}$$`
 `$$H_0: At \ least \ one \ variance \ or \ covariance \ is \ not \ equal \ to \ others.$$`
**Inference for the Marginal Model:**

- When we have non-nested models, we cannot use standard tests anymore.

- When we compare two non-nested models, we choose the model that has the lowest &lt;span style="color:purple; font-weight:bold;"&gt;AIC/BIC&lt;/span&gt; value.

- The &lt;span style="color:purple; font-weight:bold;"&gt;unstructured covariance matrix&lt;/span&gt; is the most general matrix we can assume:
  - All other covariance matrices are a special case of the unstructured matrix.
  - But realistically, it can only be fitted when we have balanced data and relatively few time points.

---

**Information Criteria:**

- LR tests can only be used to compare nested models.

- How to compare non-nested models?

- The general idea behind the &lt;span style="color:purple; font-weight:bold;"&gt;LR test&lt;/span&gt; for comparing model A to a more extensive model B is to select model A if the increase in likelihood under model B is small compared to the increase in complexity.

- A similar argument can be used to compare non-nested models A and B.

- One then selects the model with the largest (log-)likelihood provided it is not (too) complex.

- Criterion: &lt;span style="color:purple; font-weight:bold;"&gt;Akaike (AIC), Schwarz (SBC)&lt;/span&gt;.

---

**AIC:**

- For each model compute &lt;span style="color:purple; font-weight:bold;"&gt;AIC&lt;/span&gt; = `\(-2 \log(L) + kp\)`.

- `\(p\)` is the number of parameters in the model.

- `\(L\)` is the likelihood.

- `\(k\)` is a constant (often 2). `\(k\)` can be seen as a penalty for additional parameters. `\(k\)` between 2 and 6. The recommendation is to use a larger `\(k\)` with a small sample.

- Has theoretical basis in the prediction of future data.

- In practice, it sometimes overfits and chooses models that are sometimes too large.

---

**Notes on IC:**

- &lt;span style="color:darkorange; font-weight:bold;"&gt;Information criteria are not formal testing procedures!&lt;/span&gt;

- The AIC and BIC do not always select the same model – when they disagree:

  - AIC typically selects the more elaborate model, whereas BIC selects the more parsimonious model.

- For the comparison of models with different mean structures, &lt;span style="color:blue; font-weight:bold;"&gt;IC should be based on ML rather than REML&lt;/span&gt;, as otherwise the likelihood values would be based on different sets of error contrasts, and therefore would no longer be comparable.

---

**Hypothesis Testing for Regression Coefficients:**

- Hypothesis testing on `\(\beta:\)` We assume that a suitable choice for the covariance matrix has been made.

- In the majority of the cases, we compare nested models, and hence standard tests can be used.

- We distinguish between two cases:
  - Tests for individual coefficients.
  - Tests for groups of coefficients.

---

**Tests for Individual Coefficients:**

- Tests for individual coefficients are based on the Wald-type statistic but assume the t-distribution for calculating p-values.

- The hypothesis is:
  `$$H_0: \beta = 0 \quad H_a: \beta ≠ 0$$`
- We use the t test statistic: `$$\frac{{\hat{s.e.}(\beta)}}{{\hat{\beta}}} \sim t_{df}$$`

- df specified according to the number of subjects and the number of repeated measurements per subject.

---

**Tests for Group Coefficients:**

- Tests for group coefficients are based on the &lt;span style="color:purple"&gt;F-test&lt;/span&gt;.

- The hypothesis is:
  `$$H_0: L\beta = 0 \ vs \ H_a: L\beta \neq 0$$`
  where `\(L\)`&lt;/span&gt; is the contrasts matrix.

- The numerator df are always equal to the rank of the contrast matrix `\(L\)`.

- Denominator df need to be estimated from the data using methods such as the &lt;span style="color:darkorange"&gt;Containment method&lt;/span&gt;, &lt;span style="color:darkorange"&gt;Satterthwaite approximation&lt;/span&gt;, &lt;span style="color:darkorange"&gt;Kenward and Roger approximation&lt;/span&gt;.

- There is no single method that provides satisfactory results in all settings - it matters more what you do in &lt;span style="color:darkorange"&gt;small samples&lt;/span&gt;.


---

#### Notes on Hypothesis Testing for Regression Coefficients:

- Hypothesis testing for the regression coefficients `\(\beta\)`:
 
  - The &lt;span style="color:red; font-weight:bold;"&gt;likelihood ratio test&lt;/span&gt;, and the classical univariate and multivariate &lt;span style="color:red; font-weight:bold;"&gt;Wald tests&lt;/span&gt; (using the `\(\chi^2\)` distribution instead of the t or F distributions), are 'liberal'.
 
  - They give &lt;span style="color:purple; font-weight:bold;"&gt;smaller p-values&lt;/span&gt; than they should give, especially in &lt;span style="color:purple; font-weight:bold;"&gt;small samples&lt;/span&gt;.

- The LRT for comparing models with different `\(\beta\)` parts is only valid when the models have been fitted using maximum likelihood and not REML.


**For Studies with Small Samples:**

- &lt;span style="color:blue; font-weight:bold;"&gt;P-values of Wald test may be too small.&lt;/span&gt;
- &lt;span style="color:blue; font-weight:bold;"&gt;Confidence intervals may be too narrow.&lt;/span&gt;
- &lt;span style="color:red; font-weight:bold;"&gt;t and F-tests&lt;/span&gt; may be used to remedy this.
- Difficulty is with determining the df.


---

#### Inference for the Variance Components:

- Inference for the &lt;span style="color:green"&gt;mean structure&lt;/span&gt; is usually of primary interest.

- However, inferences for the &lt;span style="color:blue"&gt;covariance structure&lt;/span&gt; are of interest as well:
  - Interpretation of the **&lt;span style="color:purple"&gt;random variation&lt;/span&gt;** in the data.
  - **&lt;span style="color:purple"&gt;Over-parameterized covariance structures&lt;/span&gt;** lead to inefficient inferences for the **&lt;span style="color:green"&gt;mean&lt;/span&gt;**.
  - Too restrictive models invalidate inferences for the **&lt;span style="color:green"&gt;mean structure&lt;/span&gt;**.
  

- The reported p-values often do not test meaningful hypotheses

- The reported &lt;span style="color:blue"&gt;p-values&lt;/span&gt; are often wrong.

- The &lt;span style="color:blue"&gt;sample size requirements&lt;/span&gt; for these tests are excessive &amp; often not met (approximately 400 or more subjects).

---

#### Likelihood Ratio Test

- After a candidate model is selected, a &lt;span style="color:blue"&gt;LRT&lt;/span&gt; can be computed by comparing the candidate model with the reduced model.

- The &lt;span style="color:blue"&gt;mean structure&lt;/span&gt; of the model remains the same across both models, but the number of &lt;span style="color:blue"&gt;random effects&lt;/span&gt; is reduced by one in the reduced model.

- Note: as long as models are compared with the same &lt;span style="color:blue"&gt;mean structure&lt;/span&gt;, a valid LR test can be obtained under &lt;span style="color:blue"&gt;REML&lt;/span&gt; as well.

- Note: if `\(H_0\)` is a **&lt;span style="color:purple"&gt;boundary value&lt;/span&gt;**, the classical `\(\chi^2\)` approximation may not be valid.

- For some very specific null-hypotheses on the boundary, the correct asymptotic null-distribution has been derived.

- Example: for the &lt;span style="color:red"&gt;infant survival data&lt;/span&gt;, testing whether the **&lt;span style="color:purple"&gt;variance components&lt;/span&gt;** associated with the **&lt;span style="color:purple"&gt;random time effect&lt;/span&gt;** are equal to zero is equivalent to testing 
`$$H_0 : d_{12} = d_{22} = 0$$`

---

**&lt;span style="color:red"&gt;Case 1: No Random Effects vs One Random Effect&lt;/span&gt;**

- **&lt;span style="color:blue"&gt;Hypothesis of Interest:&lt;/span&gt;**

`\(H_0: D = 0\)` vs `\(H_a: D = d_{11}\)` for some scalar `\(d_{11}\)`

- Asymptotic null distribution equals `\(-2 \ln \lambda_N \rightarrow \chi_{0:1}^2\)`, the mixture of `\(\chi_0^2\)` and `\(\chi_1^2\)` with equal weight 0.5.


**&lt;span style="color:red"&gt;Case 2: One vs Two Random Effects&lt;/span&gt;**

- **&lt;span style="color:blue"&gt;Hypothesis of Interest:&lt;/span&gt;**

`$$D = \begin{pmatrix} d_{11} &amp; 0 \\ 0 &amp; 0 \end{pmatrix},$$` for `\(d_{11} &gt; 0\)`, versus `\(H_a\)` that `\(D\)` is a 2 by 2 positive semi definite matrix.

- Asymptotic null distribution: `\(-2 \ln \lambda_N \rightarrow \chi_{1:12}^2\)`, the mixture of `\(\chi_1^2\)` and `\(\chi_2^2\)` with equal weight 0.5.

---
class: inverse,  middle

# Day 3

## Models for Non-Gaussian Longitudinal Data
 
 * Generalized Estimating Equations (GEE)
 * Generalized Linear Mixed Model (GLMM)
 
---

#### Recap: Marginal (population average) models:

- Responses are marginalized over all other responses.

- Parameters characterize the &lt;span style="color:purple; font-weight:bold;"&gt;marginal expectation&lt;/span&gt;.

- Inferences based on the marginal model do not explicitly assume the presence of &lt;span style="color:blue; font-weight:bold;"&gt;random effects&lt;/span&gt; representing the natural heterogeneity between subjects.

- The implied marginal model equals: `$$\mathbf{Y}_i \sim \mathcal{N}(\mathbf{X}_i\beta, \mathbf{Z}_iD\mathbf{Z}_i' + \Sigma_i).$$`

#### Subject-specific models:

- If the aim is to study how subjects change overtime and what characteristics influence such changes.

- Subject-specific models differ from marginal models by the inclusion of parameters specific to the subject.
`$$\mathbf{Y}_i|\mathbf{b}_i \sim \mathcal{N}(\mathbf{X}_i\beta + \mathbf{Z}_i\mathbf{b}_i,\, \Sigma_i),$$`
  where `\(\mathbf{b}_i \sim \mathcal{N}(0, \mathbf{D}_i).\)`

---

**Recap: Generalized linear models**

- LMM have the assumption that the conditional responses are normally distributed.

- Normality assumption may not always be &lt;span style="color:purple; font-weight:bold;"&gt;reasonable&lt;/span&gt;, i.e., &lt;span style="color:purple; font-weight:bold;"&gt;non-Gaussian responses&lt;/span&gt;.

- Different methodology should be used when responses are &lt;span style="color:purple; font-weight:bold;"&gt;discrete&lt;/span&gt;.

- Suppose we have a dichotomous outcome, Y, measured cross-sectionally.

- We are interested in making statistical inferences for this outcome, e.g.:

  - Is there any difference between placebo and treatment corrected for the age and sex of the patients?
  - Which factors best predict the outcome?

---

**Generalized linear models**

- Suppose we have a dichotomous outcome, Y, measured cross-sectionally.
`$$\log\left(\frac{{\text{odds of success}}}{{1 - \text{odds of success}}}\right) = \log(\pi_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}$$`
- Odds of success = `\(\pi_i / (1 - \pi_i) = \exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip})\)`

- A unit change in `\(X_1\)` from `\(x\)` to `\(x+1\)` (while all other covariates are held fixed) corresponds to `\(\exp(\beta_1)\)`.


**Example Toenail Data**

- **Toenail Dermatophyte Onychomycosis:** Common toenail infection, difficult to treat, affecting more than **&lt;span style="color:purple"&gt;2%&lt;/span&gt;** of the population.

- Classical treatments with **&lt;span style="color:red"&gt;antifungal compounds&lt;/span&gt;** need to be administered until the whole nail has grown out healthy.

- New compounds have been developed which reduce treatment to **&lt;span style="color:purple"&gt;3 months&lt;/span&gt;**.

- Randomized, double-blind, parallel group, multicenter study.

---

**Toenail Data**

- **Research question:** **&lt;span style="color:red"&gt;Severity relative to treatment of TDO, coded as  0 (not severe) or 1 (severe)&lt;/span&gt;**.

- The question of interest was whether the **&lt;span style="color:blue"&gt;percentage of severe infection decreased&lt;/span&gt;** over time, and whether that evolution was different for the **&lt;span style="color:blue"&gt;two treatment groups&lt;/span&gt;**:
 
  - &lt;span style="color:red"&gt;2 × 189 patients&lt;/span&gt; randomized, &lt;span style="color:red"&gt;36 centers&lt;/span&gt;.
 
  - &lt;span style="color:red"&gt;48 weeks&lt;/span&gt; of total follow-up (&lt;span style="color:red"&gt;12 months&lt;/span&gt;).
 
  - &lt;span style="color:red"&gt;12 weeks&lt;/span&gt; of treatment (&lt;span style="color:blue"&gt;3 months&lt;/span&gt;).
 
  - Measurements at &lt;span style="color:blue"&gt;months 0, 1, 2, 3, 6, 9, 12&lt;/span&gt;.
  
---

**Toenail Data**


```r
library(readr)
toenail &lt;- read_csv("Data/Toenail.csv")
```


```r
library(gtsummary)
toenail %&gt;% select(time, treatn) %&gt;% 
  tbl_summary(by=treatn) 
```

<div id="lhwctscxvs" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#lhwctscxvs table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#lhwctscxvs thead, #lhwctscxvs tbody, #lhwctscxvs tfoot, #lhwctscxvs tr, #lhwctscxvs td, #lhwctscxvs th {
  border-style: none;
}

#lhwctscxvs p {
  margin: 0;
  padding: 0;
}

#lhwctscxvs .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#lhwctscxvs .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#lhwctscxvs .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#lhwctscxvs .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#lhwctscxvs .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#lhwctscxvs .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#lhwctscxvs .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#lhwctscxvs .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#lhwctscxvs .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#lhwctscxvs .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#lhwctscxvs .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#lhwctscxvs .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#lhwctscxvs .gt_spanner_row {
  border-bottom-style: hidden;
}

#lhwctscxvs .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#lhwctscxvs .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#lhwctscxvs .gt_from_md > :first-child {
  margin-top: 0;
}

#lhwctscxvs .gt_from_md > :last-child {
  margin-bottom: 0;
}

#lhwctscxvs .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#lhwctscxvs .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#lhwctscxvs .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#lhwctscxvs .gt_row_group_first td {
  border-top-width: 2px;
}

#lhwctscxvs .gt_row_group_first th {
  border-top-width: 2px;
}

#lhwctscxvs .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#lhwctscxvs .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#lhwctscxvs .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#lhwctscxvs .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#lhwctscxvs .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#lhwctscxvs .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#lhwctscxvs .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#lhwctscxvs .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#lhwctscxvs .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#lhwctscxvs .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#lhwctscxvs .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#lhwctscxvs .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#lhwctscxvs .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#lhwctscxvs .gt_left {
  text-align: left;
}

#lhwctscxvs .gt_center {
  text-align: center;
}

#lhwctscxvs .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#lhwctscxvs .gt_font_normal {
  font-weight: normal;
}

#lhwctscxvs .gt_font_bold {
  font-weight: bold;
}

#lhwctscxvs .gt_font_italic {
  font-style: italic;
}

#lhwctscxvs .gt_super {
  font-size: 65%;
}

#lhwctscxvs .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#lhwctscxvs .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#lhwctscxvs .gt_indent_1 {
  text-indent: 5px;
}

#lhwctscxvs .gt_indent_2 {
  text-indent: 10px;
}

#lhwctscxvs .gt_indent_3 {
  text-indent: 15px;
}

#lhwctscxvs .gt_indent_4 {
  text-indent: 20px;
}

#lhwctscxvs .gt_indent_5 {
  text-indent: 25px;
}
</style>
<table class="gt_table" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
  <thead>
    
    <tr class="gt_col_headings">
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1" scope="col" id="&lt;strong&gt;Characteristic&lt;/strong&gt;"><strong>Characteristic</strong></th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&lt;strong&gt;0&lt;/strong&gt;, N = 937&lt;span class=&quot;gt_footnote_marks&quot; style=&quot;white-space:nowrap;font-style:italic;font-weight:normal;&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;"><strong>0</strong>, N = 937<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;"><sup>1</sup></span></th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="&lt;strong&gt;1&lt;/strong&gt;, N = 970&lt;span class=&quot;gt_footnote_marks&quot; style=&quot;white-space:nowrap;font-style:italic;font-weight:normal;&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/span&gt;"><strong>1</strong>, N = 970<span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;"><sup>1</sup></span></th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td headers="label" class="gt_row gt_left">time</td>
<td headers="stat_1" class="gt_row gt_center"></td>
<td headers="stat_2" class="gt_row gt_center"></td></tr>
    <tr><td headers="label" class="gt_row gt_left">    0</td>
<td headers="stat_1" class="gt_row gt_center">146 (16%)</td>
<td headers="stat_2" class="gt_row gt_center">148 (15%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">    1</td>
<td headers="stat_1" class="gt_row gt_center">141 (15%)</td>
<td headers="stat_2" class="gt_row gt_center">147 (15%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">    2</td>
<td headers="stat_1" class="gt_row gt_center">138 (15%)</td>
<td headers="stat_2" class="gt_row gt_center">145 (15%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">    3</td>
<td headers="stat_1" class="gt_row gt_center">132 (14%)</td>
<td headers="stat_2" class="gt_row gt_center">140 (14%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">    6</td>
<td headers="stat_1" class="gt_row gt_center">130 (14%)</td>
<td headers="stat_2" class="gt_row gt_center">133 (14%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">    9</td>
<td headers="stat_1" class="gt_row gt_center">117 (12%)</td>
<td headers="stat_2" class="gt_row gt_center">126 (13%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">    12</td>
<td headers="stat_1" class="gt_row gt_center">133 (14%)</td>
<td headers="stat_2" class="gt_row gt_center">131 (14%)</td></tr>
  </tbody>
  
  <tfoot class="gt_footnotes">
    <tr>
      <td class="gt_footnote" colspan="3"><span class="gt_footnote_marks" style="white-space:nowrap;font-style:italic;font-weight:normal;"><sup>1</sup></span> n (%)</td>
    </tr>
  </tfoot>
</table>
</div>

---
**Recap: Generalized linear models**



```r
fit &lt;- glm(formula = y ~ treatn * time, family = binomial, data = toenail)
summary(fit)$coefficients
```

```
##                Estimate Std. Error    z value     Pr(&gt;|z|)
## (Intercept) -0.55705808 0.10903934 -5.1087806 3.242445e-07
## treatn       0.02357666 0.15648006  0.1506688 8.802370e-01
## time        -0.17692959 0.02455777 -7.2046268 5.820304e-13
## treatn:time -0.07797568 0.03943628 -1.9772574 4.801254e-02
```

- `\(\beta_3 = -0.078\)` is the interaction effect.
- Borderline significance (p = 0.048) which means that trends might be different in the two treatment groups.
- This analysis is wrong since we have not taken into account the multiple responses per subject.
- We have considered 1907 independent observations (residual degrees of freedom + 1).

```r
length(toenail$y)
```

```
## [1] 1907
```

---

### Generalized Estimating Equations (GEE)

- We return our focus on repeated measurements data, namely, repeated categorical data
  - we need to account for the correlations

**Reminder:** In the marginal models for continuous multivariate data we took account of the correlations by incorporating a correlation matrix in the error terms
  - &lt;span style="color:blue;"&gt;ML&lt;/span&gt; and &lt;span style="color:red;"&gt;REML&lt;/span&gt;

**Challenges for Non-Gaussian Data**

- For non-Gaussian data it is not straightforward to do that because there are no clear multivariate analogues of the univariate distributions
  - we will do something similar, not in the error terms but in the score equations

- Popular alternative approaches, like &lt;span style="color:blue;"&gt;GEEs&lt;/span&gt;, Alternating logistic regression (ALR), and Pseudo-likelihood (PL) have been formulated.
---

#### GEE .....(2)

- GEE introduced by Liang and Zeger (Biometrika, 1986) is a &lt;span style="color:blue"&gt;best way&lt;/span&gt; to model longitudinal data in the marginal modeling framework for categorical responses.

- The parameters of &lt;span style="color:blue"&gt;GLMs&lt;/span&gt; are estimated using the maximum likelihood approach.
- Key idea: Finding the top of the &lt;span style="color:blue"&gt;log-likelihood mountain&lt;/span&gt; is equivalent to finding the parameter values for which the slope of the mountain is flat (i.e., zero).
- The slope of the &lt;span style="color:blue"&gt;log-likelihood mountain&lt;/span&gt; is given by the score vector:

`$$S_{\beta} = \sum_{i} \frac{\partial \mu_i}{\partial \beta} V_i^{-1} (Y_i - \mu_i)$$`

Where:
- `\(S_{\beta}\)` represents the score vector.
- `\(\mu_i\)` is the mean of `\(Y_i\)`, and for dichotomous data `\(\mu_i = \pi_i\)`.
- `\(V_i\)` is a diagonal matrix with the variance of `\(Y_i\)`, e.g., for dichotomous data `\(V_{i-1} = diag(\pi_i(1 - \pi_i))\)`.

---

The idea of Liang and Zeger was to replace the diagonal matrix `\(V_i\)` with a full covariance matrix:

`$$V_i = A_i^{-1/2} R_i(\alpha) A_i^{-1/2}$$`

Where:
- `\(A_i\)` is a diagonal matrix with the standard deviations `\(\sqrt{var(Y_i)}\)`.
- `\(R_i(\alpha)\)` represents a 'working' assumption for the pairwise correlations.

- This approach follows the same form as the full likelihood procedure but restricts the specification to the first moment only.
---
#### GEE....(3)

- If the assumed mean structure, `\(\mu_i\)`, is correctly specified, then
  `$$\hat{\beta} \sim \mathcal{N}(\beta, \text{var}(\hat{\beta}))$$`
  where 
  `\(\text{var}(\hat{\beta}) = V_0^{-1} V_1 V_0^{-1}\)`
  is called the Sandwich or Robust estimator.
  - `\(V_0 = \sum_i \frac{\partial \mu_i}{\partial \beta} V_i^{-1} \frac{\partial \mu_i}{\partial \beta}\)`
  - `\(V_1 = \sum_i \frac{\partial \mu_i}{\partial \beta} \text{var}(Y) V_1^{-1} \frac{\partial \mu_i}{\partial \beta}\)`

   `\(V_0\)` and `\(V_1\)` are often referred to as the "bread" and "meat" of the Sandwich estimator.

- GEE provides consistent regression coefficient estimates even if the correlation structure is miss-specified.

- A poor choice of working correlation matrix can affect the efficiency of the estimators of `\(\beta\)`.

---

**Sandwich/Robust vs Naive/Purely Model-Based Standard Errors**

- Software often also reports the Naive/model-based standard errors.
- These standard errors assume that the working correlation matrix is correctly specified.
- The Sandwich/Empirically Corrected/Robust standard errors correct for a possible misspecification of the correlation structure, although at the expense of power.
- A correct guess ⟹ likelihood variance.

- &lt;span style="color:blue"&gt;GEE is not a likelihood-based approach&lt;/span&gt; (i.e., a model)
  - It is an estimation method
- No assumptions for the joint distribution of repeated measurements ⟹ &lt;span style="color:blue"&gt;Semi-parametric approach&lt;/span&gt;
- The method relies solely on assumptions about the mean response
  - Pairwise correlations ⇒ we make a "&lt;span style="color:blue"&gt;working&lt;/span&gt;" assumption that possibly depends on parameters to be estimated
  
---

- The mean and the correlations are separately defined! 
   - This is in contrast to the GLMMs we will see in the next class.

- &lt;span style="color:blue"&gt;**Fitting algorithm**&lt;/span&gt;:

  - Fit a Generalized linear model ⟹ **&lt;span style="color:blue"&gt;choose a working correlation matrix&lt;/span&gt;** ⟹ update `\(\hat{\beta}\)`, the covariance, and the correlation matrix.

- Interest is primarily in the `\(\beta\)`s, the covariance structure is considered as “nuisance” 

   – Assumptions for the correlation are not supposed to be correct.

- This has implications for &lt;span style="color:blue"&gt;Hypothesis testing&lt;/span&gt;:

  - Likelihood ratio test or score test not applicable. 
  
  Why? (semi-parametric approach)   ⇒ The Wald test can be used.

- Care needed with **&lt;span style="color:blue"&gt;incomplete data&lt;/span&gt;**.

---

## GEE in R

- In R there are two main packages for GEE analysis, namely &lt;span style="color:blue"&gt;gee&lt;/span&gt; and &lt;span style="color:blue"&gt;geepack&lt;/span&gt;.

- The main function to fit GEEs is &lt;span style="color:blue"&gt;geeglm()&lt;/span&gt; – this has similar syntax as the &lt;span style="color:blue"&gt;glm()&lt;/span&gt; function of base R that fits GLMs.

- The major difference between &lt;span style="color:blue"&gt;gee&lt;/span&gt; and &lt;span style="color:blue"&gt;geepack&lt;/span&gt; is that &lt;span style="color:blue"&gt;geepack&lt;/span&gt; contains an &lt;span style="color:blue"&gt;ANOVA method&lt;/span&gt; that allows us to compare models and perform &lt;span style="color:blue"&gt;Wald tests&lt;/span&gt;.

**Using Toenail data:** Variables in the data:
- **obs:** observation number
- **treat:** treatment group (0: Itraconazole (group B); 1: Lamisil (group A))
- **id:** subject identification number
- **time:** time at which the observation is taken (months)
- **response:** the response measured (1: severe infection; 0: no severe infection)

**Research question:** Does treatment have an effect in curing the infection or not?

---

A function that fits GEE to deal with correlation structures arising from repeated measures on individuals, or from clustering as in family data is:


```r
gee(formula, family, data, corStructure = "ar1", clusterID, startCoeff, 
    maxit = 20, checks = TRUE, display = FALSE, datasources)
```

- `formula`: a string character which describes the model to be fitted.

- `family`: description of the error distribution: 'binomial', 'gaussian', 'Gamma', or 'poisson'.

- `data`: the name of the data frame that holds the variables.

- `corStructure`: the correlation structure: 'ar1', 'exchangeable', 'independence', 'fixed', or 'unstructured'.

- `clusterID`: the name of the column that holds the cluster IDs.

- `startCoeff`: a numeric vector, the starting values for the beta coefficients.

- `maxit`: an integer, the maximum number of iterations to use for convergence.

---

To fit GEE in R, you need the following packages first: **&lt;span style="color:red"&gt;geepack, wgeesel, MuMIn&lt;/span&gt;**


```r
library(geepack)
library(broom)
# Fit the GEE models
fit1 &lt;- geeglm(y ~ treatn + time + treatn*time, id = idnum, data = toenail,
               family = binomial(link = "logit"), corstr = "exchangeable", scale.fix = TRUE)
fit2 &lt;- update(fit1, corstr = "ar1")
fit3 &lt;- update(fit2, corstr = "unstructured")

# Obtain summary results using broom
fit1_summary &lt;- tidy(fit1)
fit2_summary &lt;- tidy(fit2)
fit3_summary &lt;- tidy(fit3)

# Print or manipulate the customized summaries as needed
print(fit1_summary)
```

```
## # A tibble: 4 × 5
##   term        estimate std.error statistic      p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept) -0.587      0.174   11.4     0.000731    
## 2 treatn       0.00840    0.262    0.00103 0.974       
## 3 time        -0.177      0.0312  32.2     0.0000000136
## 4 treatn:time -0.0871     0.0570   2.34    0.126
```
---

```r
print(fit2_summary)
```

```
## # A tibble: 4 × 5
##   term        estimate std.error statistic     p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;
## 1 (Intercept)   -0.644    0.170     14.4   0.000151   
## 2 treatn         0.115    0.250      0.212 0.645      
## 3 time          -0.143    0.0285    25.2   0.000000518
## 4 treatn:time   -0.116    0.0550     4.46  0.0346
```

```r
print(fit3_summary)
```

```
## # A tibble: 4 × 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)  -0.800     0.170     22.1   0.00000255
## 2 treatn        0.0980    0.259      0.143 0.705     
## 3 time         -0.122     0.0268    20.9   0.00000486
## 4 treatn:time  -0.135     0.0537     6.31  0.0120
```


```r
broom:::confint.geeglm(fit1)
```

```
##                    lwr         upr
## (Intercept) -0.9269824 -0.24621415
## treatn      -0.5050685  0.52187849
## time        -0.2383974 -0.11606084
## treatn:time -0.1988485  0.02456112
```

---

#### Choosing the best model
- QIC = quasi-likelihood under the independence model criterion

- GEE does not use maximum likelihood estimation like GLMM

- QIC can help select working correlation matrix in GEE

- MuMIn package calculates this statistic


```r
#library(MuMIn)
QIC(fit1, fit2, fit3)
```

```
##           QIC     QICu Quasi Lik      CIC params     QICC
## fit1 1834.248 1820.574 -906.2870 10.83699      4 1834.456
## fit2 1833.913 1821.771 -906.8853 10.07134      4 1834.122
## fit3 1842.298 1829.939 -910.9694 10.17942      4 1847.148
```


---

#### &lt;span style="color:red"&gt;Generalized Linear Mixed Models (GLMMs)&lt;/span&gt;

- &lt;span style="color:blue"&gt;GLMMs = GLMs (Logistic, Poisson, etc) with random effects&lt;/span&gt;.

- The intuitive idea behind GLMMs is the same as in LMMs, i.e.,

  - The correlation between the repeated categorical measurements is induced by &lt;span style="color:blue"&gt;unobserved random effects&lt;/span&gt;.

  - The categorical longitudinal measurements of a subject are correlated because all of them share the same &lt;span style="color:blue"&gt;unobserved random effect&lt;/span&gt; (conditional independence assumption).

- The generic mixed model for `\(y_{ij}\)` is a &lt;span style="color:blue"&gt;Mixed-Effects Logistic Regression&lt;/span&gt; and has the form:

`$$\log\left(\frac{\pi_{i}}{1 - \pi_{i}}\right) = X_{i}\beta + Z_{i}b_{i}, \quad b_{i} \sim \mathcal{N}(0, D)$$`
- **&lt;span style="color:red"&gt;Random effects&lt;/span&gt;** account for between-subject variability.

---

**Three-part specification for GLMMs**

&lt;span style="color:blue"&gt;**1.**&lt;/span&gt; Conditional on the random effects `\(b_{i}\)`, the responses `\(y_{ij}\)` are independent and have a Bernoulli distribution with mean `\(E(y_{ij} / b_{i}) = \pi_{ij}\)` and variance `\(\text{Var}(y_{ij} / b_{i}) = \pi_{ij} (1 - \pi_{ij})\)`

&lt;span style="color:blue"&gt;**2.**&lt;/span&gt; The conditional mean of `\(y_{ij}\)` is given as `\(\log\left(\frac{\pi_{ij}}{1 - \pi_{ij}}\right) = x_{i}^T\beta + z_{i}^Tb_{i}\)`

&lt;span style="color:blue"&gt;**3.**&lt;/span&gt; The random effects `\(\sim N(0, D\)`.

- The mean and correlation structures are &lt;span style="color:blue"&gt;simultaneously defined using random effects&lt;/span&gt;.
    - This has direct and important implications with respect to the interpretation of the parameters!

---

#### &lt;span style="color:red"&gt;Estimation of GLMMs&lt;/span&gt;

- The estimation of GLMMs is based on the same principles as in marginal and mixed models for continuous data.
 
  - i.e., we have a full specification of the distribution of the data (contrary to GEE), and hence we can use &lt;span style="color:blue"&gt;maximum likelihood&lt;/span&gt;.

- No &lt;span style="color:blue"&gt;REML&lt;/span&gt;.

- Nevertheless, there is an important complication in GLMMs.

- The fitting of GLMMs is a &lt;span style="color:blue"&gt;computationally challenging task&lt;/span&gt;!

---

#### Log-likelihood expression for GLMMs

- What is the problem?
- The log-likelihood expression for GLMMs has the same form as in LMMs:
`$$\ell(\theta) = \sum_{i=1}^n \int p(y_{i} / b_{i}; \theta) p(b_{i}; \theta) db_{i}$$`
where `\(\theta\)` are the parameters of the model.
- In linear mixed effects models, both terms in the integrand 
    - `\(p(y_{i} / b_{i}; \theta)\)` and  `\(p(b_{i}; \theta)\)` 

are densities of (multivariate) normal distributions, and also because `\(y_{i}\)` and `\(b_{i}\)` are linearly related.


---

- **&lt;span style="color:blue"&gt;What is the problem?&lt;/span&gt;**

- In GLMMs, the two terms of the integrand denote densities of different distributions. For example, in mixed effects logistic regression:
 
  - `\(p(y_{i} / b_{i}; \theta)\)` ⟹ Bernoulli distribution
 
  - `\(p(b_{i}; \theta)\)` ⟹ Multivariate Normal distribution

- The implication is that in GLMMs, the same integral does &lt;span style="color:blue"&gt;not have a closed-form solution&lt;/span&gt;.

**&lt;span style="color:blue"&gt;The Solutions&lt;/span&gt;**

To overcome this problem, two general types of solutions have been proposed:

- **Approximation of the integrand:** This entails approximating the product inside the integral (i.e., `\(p(y_{i} / b_{i}; \theta) p(b_{i}; \theta)\)`) by a multivariate normal distribution for which the integral has a closed-form solution:
  - &lt;span style="color:blue"&gt;Penalized Quasi Likelihood (PQL)&lt;/span&gt;
  - &lt;span style="color:blue"&gt;Laplace approximation&lt;/span&gt;

---

- **Approximation of the integral:** This entails approximating the whole integral (i.e., `\(\int p(y_{i} / b_{i}; \theta) p(b_{i}; \theta)\)`) by a sum:
  - &lt;span style="color:blue"&gt;Gaussian Quadrature (GQ) &amp; Adaptive Gaussian Quadrature (AGQ)&lt;/span&gt;
  - &lt;span style="color:blue"&gt;Monte Carlo &amp; MCMC (Bayesian approach)&lt;/span&gt;

- From the two alternatives, methods that rely on **&lt;span style="color:red"&gt;approximation of the integral (GQ, AGQ) have been shown to be superior&lt;/span&gt;**.

- Though they are (much) more computationally demanding, they have a parameter that controls the accuracy of the approximation:
 
  - **&lt;span style="color:purple"&gt;In GQ rules, it is the number of quadrature points (nGQ=1) point is equivalent to the Laplace approximation.&lt;/span&gt;**
 
  - **&lt;span style="color:red"&gt;AGQ needs fewer quadrature points than classical GQ but is more time-consuming.&lt;/span&gt;**
 
  - **&lt;span style="color:green"&gt; The Laplace approximation is a good choice when dealing with many repeated measures per subject.&lt;/span&gt;**

- **&lt;span style="color:purple"&gt;The higher nGQ (nAGQ), the more accurate the approximation will be.&lt;/span&gt;**

---

#### Estimation of random effects in GLMMs

- Estimation of the random effects proceeds in a similar manner as in linear mixed models.

- Predictions of random effects can be based on the &lt;span style="color:blue"&gt;posterior distribution&lt;/span&gt; `\(f(b_{i} / Y_{i} = y_{i})\)`.

  - &lt;span style="color:blue"&gt;Empirical Bayes (EB) estimate&lt;/span&gt;:

    - May be used when interest is in predicting subject-specific evolutions.
    - Identifying subjects with outlying evolutions.
  - Estimation is based on the posterior distribution of `\(b_{i}\)`.
  - &lt;span style="color:blue"&gt;Posterior mode&lt;/span&gt; used as estimate.

- With EB estimates, `\(\hat{b}_{i}\)`, subject-specific probability profile:
  `$$P(\widehat{Y_{ij} = 1} | b_{i}) = \frac{\exp(X_{ij}\hat{\beta} + Z_{ij}\hat{b}_{i})}{1 + \exp(X_{ij}\hat{\beta} + Z_{ij}\hat{b}_{i})}$$`

---
#### Conditional interpretation of `\(\beta\)` in GLMMs

- `\(\beta\)` in GLMMs has a **&lt;span style="color:blue"&gt;conditional interpretation&lt;/span&gt;**.

- The parameters are **&lt;span style="color:red"&gt;conditional on the random effects&lt;/span&gt;**.

- Interpretation of the &lt;span style="color:blue"&gt;fixed-effects coefficients&lt;/span&gt;:

  - For example, `\(e^\beta\)` does not have the interpretation of the average Odds Ratio (OR) for a unit increase in follow-up.

  - The parameters are **&lt;span style="color:red"&gt;conditional on the random effects&lt;/span&gt;**.
- Considers the effect of predictors while accounting for variability between groups due to random effects.

---

#### GLMMs in R


- **Packages: &lt;span style="color:red"&gt;lme4&lt;/span&gt; and &lt;span style="color:red"&gt;GLMMadaptive&lt;/span&gt;**

- The function that fits GLMMs in &lt;span style="color:red"&gt;lme4&lt;/span&gt; is `glmer()` 
  - this has similar syntax as the `lmer()` function that fits linear mixed models, namely:

  - `formula`: specifying the &lt;span style="color:red"&gt;response vector&lt;/span&gt;, the &lt;span style="color:red"&gt;fixed- and random-effects structure&lt;/span&gt;

  - `data`: a &lt;span style="color:red"&gt;data frame&lt;/span&gt; containing all the variables

  - `family`: specifying the &lt;span style="color:red"&gt;distribution of the outcome&lt;/span&gt; and the &lt;span style="color:red"&gt;link function&lt;/span&gt;

  - `nAGQ`: the &lt;span style="color:red"&gt;number of quadrature points&lt;/span&gt;

---

#### GLMMs in R: lme4

- Fits a mixed effects logistic regression for Toenail data with random intercepts and 15 quadrature points for the adaptive Gauss-Hermite rule:
---


```r
library(lme4)
glmmFit &lt;- glmer(y ~ treatn*time + (1 | idnum), family = binomial(), 
                 data = toenail, nAGQ = 15)
summary(glmmFit)
```

```
## Generalized linear mixed model fit by maximum likelihood (Adaptive
##   Gauss-Hermite Quadrature, nAGQ = 15) [glmerMod]
##  Family: binomial  ( logit )
## Formula: y ~ treatn * time + (1 | idnum)
##    Data: toenail
## 
##      AIC      BIC   logLik deviance df.resid 
##   1257.1   1284.9   -623.6   1247.1     1902 
## 
## Scaled residuals: 
##    Min     1Q Median     3Q    Max 
## -2.963 -0.189 -0.087 -0.007 38.155 
## 
## Random effects:
##  Groups Name        Variance Std.Dev.
##  idnum  (Intercept) 16.52    4.065   
## Number of obs: 1907, groups:  idnum, 294
## 
## Fixed effects:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.65962    0.44596  -3.721 0.000198 ***
## treatn      -0.12255    0.59379  -0.206 0.836484    
## time        -0.40569    0.04619  -8.783  &lt; 2e-16 ***
## treatn:time -0.15962    0.07222  -2.210 0.027086 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Correlation of Fixed Effects:
##             (Intr) treatn time  
## treatn      -0.642              
## time        -0.157  0.212       
## treatn:time  0.200 -0.297 -0.554
```

---

#### GLMMs in R: GLMMadaptive

- The function that fits GLMMs in &lt;span style="color:blue"&gt;GLMMadaptive&lt;/span&gt; is `mixed_model()`. To fit the same model as we did above with `glmer()`, the code is:


```r
library(GLMMadaptive)
glmmFit2 &lt;- mixed_model(y ~ treatn*time, random = ~ 1 | idnum, 
             family = binomial(), data = toenail, nAGQ = 15)
summary(glmmFit2)
```

```
## 
## Call:
## mixed_model(fixed = y ~ treatn * time, random = ~1 | idnum, data = toenail, 
##     family = binomial(), nAGQ = 15)
## 
## Data Descriptives:
## Number of Observations: 1907
## Number of Groups: 294 
## 
## Model:
##  family: binomial
##  link: logit 
## 
## Fit statistics:
##    log.Lik      AIC      BIC
##  -623.5842 1257.168 1275.586
## 
## Random effects covariance matrix:
##               StdDev
## (Intercept) 3.983747
## 
## Fixed effects:
##             Estimate Std.Err z-value  p-value
## (Intercept)  -1.5892  0.4076 -3.8989  &lt; 1e-04
## treatn       -0.1268  0.5530 -0.2293 0.818644
## time         -0.4037  0.0459 -8.7997  &lt; 1e-04
## treatn:time  -0.1582  0.0716 -2.2094 0.027145
## 
## Integration:
## method: adaptive Gauss-Hermite quadrature rule
## quadrature points: 15
## 
## Optimization:
## method: hybrid EM and quasi-Newton
## converged: TRUE
```

---

#### GLMMs in R

- Differences between &lt;span style="color:blue"&gt;glmer()&lt;/span&gt; (package &lt;span style="color:blue"&gt;lme4&lt;/span&gt;) and &lt;span style="color:blue"&gt;mixed_model()&lt;/span&gt; (package &lt;span style="color:blue"&gt;GLMMadaptive&lt;/span&gt;):

- &lt;span style="color:blue"&gt;glmer()&lt;/span&gt; only provides the adaptive Gaussian quadrature rule for the random intercepts case, whereas &lt;span style="color:blue"&gt;mixed_model()&lt;/span&gt; uses this integration method with several random terms.

- &lt;span style="color:blue"&gt;mixed_model()&lt;/span&gt; currently only handles a single grouping factor for the random effects, i.e., you cannot fit nested or crossed random effects, whereas such designs can be fitted with &lt;span style="color:blue"&gt;glmer()&lt;/span&gt;.

- &lt;span style="color:blue"&gt;mixed_model()&lt;/span&gt; can fit zero-inflated Poisson and negative binomial data, allowing for random effects in the zero part.

---

#### Model building

- Model building for &lt;span style="color:blue"&gt;GLMMs&lt;/span&gt; proceeds in the same manner as for &lt;span style="color:blue"&gt;LMMs&lt;/span&gt;, i.e.:

- We start with an elaborate specification of 
 
  - the &lt;span style="color:blue"&gt;fixed-effects structure&lt;/span&gt; that contains all the variables we wish to study, and 
 
  - potential &lt;span style="color:blue"&gt;nonlinear&lt;/span&gt; and &lt;span style="color:blue"&gt;interaction terms&lt;/span&gt;.

- Following that, we build up the &lt;span style="color:blue"&gt;random-effects structure&lt;/span&gt;, 
 
   - starting from &lt;span style="color:blue"&gt;random intercepts&lt;/span&gt;, and then potentially 
 
   - including &lt;span style="color:blue"&gt;random slopes&lt;/span&gt;, &lt;span style="color:blue"&gt;quadratic slopes&lt;/span&gt;, etc.


- At each step, we perform **&lt;span style="color:red"&gt;Likelihood Ratio Tests (LRTs)&lt;/span&gt;** to determine if including the additional random effect improves the fit of the model.

---

#### Model Building----(2)

- After choosing the &lt;span style="color:blue"&gt;random-effects structure&lt;/span&gt;, we return to the &lt;span style="color:blue"&gt;fixed effects&lt;/span&gt; and assess whether the specification can be &lt;span style="color:red"&gt;simplified&lt;/span&gt;.

- Once again, we start by testing &lt;span style="color:blue"&gt;complex terms&lt;/span&gt; 
   - i.e., interactions and nonlinear terms), and 
   
   - then proceed to **&lt;span style="color:red"&gt;drop explanatory variables&lt;/span&gt;** if required.

- In practice, quite often, and especially for &lt;span style="color:blue"&gt;dichotomous data&lt;/span&gt;, extending the &lt;span style="color:blue"&gt;random-effects structure&lt;/span&gt; may lead to 
&lt;span style="color:blue"&gt;numerical/computational problems&lt;/span&gt;

  – This is because &lt;span style="color:blue"&gt;dichotomous data&lt;/span&gt; contain the least amount of information

- Hence, for &lt;span style="color:blue"&gt;dichotomous data&lt;/span&gt; and when we have few to moderate number of **&lt;span style="color:red"&gt;repeated measurements&lt;/span&gt;** per subject, we often can only fit **&lt;span style="color:blue"&gt;random intercepts models&lt;/span&gt;**

---

#### Pros and Cons of GLMMs

**&lt;span style="color:red"&gt; Advantages&lt;/span&gt;**

- Possible to have a more complex variance component
  structures (multi-level models, random regression).

- Fully specified models allow for e.g. power calculations.

- Likelihood inference inherently handles data that are missing at random optimally.

**&lt;span style="color:red"&gt; Drawbacks&lt;/span&gt;**

- More model assumptions, thus &lt;span style="color:blue"&gt;higher risk of misspecification&lt;/span&gt;.

- &lt;span style="color:blue"&gt;Impossible to check assumptions about the random effects&lt;/span&gt;.

- **Computationally infeasible when the number of random effects or the overall size of the data becomes large.**

---

#### Comparision of GEE and GLMM

**Normally distributed outcomes:**

- Variance component model with &lt;span style="color:blue"&gt;random intercept&lt;/span&gt; is the same
  as a &lt;span style="color:blue"&gt;repeated measurements model&lt;/span&gt; with compound symmetry
  covariance pattern.

**Other-than-normal type outcomes:**

- &lt;span style="color:red"&gt;GEE&lt;/span&gt; and &lt;span style="color:red"&gt;GLMMs&lt;/span&gt; are inherently different statistical methods!

- They differ in &lt;span style="color:red"&gt;interpretation&lt;/span&gt;.

- They differ in actual figures, i.e. &lt;span style="color:red"&gt;estimates&lt;/span&gt; and &lt;span style="color:red"&gt;SEs&lt;/span&gt;.

---

**Comparison using the Jimma Infant Data**

The response variable is categorized body mass index.

`$$Y_{ij} = \begin{cases}
1 &amp; \text{if } weight \leq 2500 \\
0 &amp; \text{otherwise}
\end{cases}$$`

The following model is assumed for the mean structure:

`$$Y_{ij} | b_i \sim \text{Bernoulli}(\pi_{ij}), \text{ for subject } i \text{ and measurement } j,$$`
Exchangeable correlation (or CS)

`$$Y_{ij} \sim \text{Bernoulli}(\pi_{ij})$$`
`$$\text{logit}(\pi_{ij}) = \color{blue}{\beta_0} + \color{green}{\beta_1 Age_{ij}} + \color{red}{\beta_2 Gender_i} + \color{purple}{\beta_3 Gender_i Age_{ij}}$$`

`\(Gender_i\)` is a gender indicator.
`\(Age_{ij}\)` is age of the `\(i^{th}\)` infant at time `\(j\)` (also the time variable).

---

**Fitting GEE model**

- using **unstructured** working corelation

```r
fit1 &lt;- geeglm(BMIBIN ~ sex + age + sex * age, id = ind, data = Infant, 
               family = binomial, corstr = "exchangeable", scale.fix = TRUE)
```

**Fitting GLMM model**

- `\(Y_{ij} | b_i \sim \text{Bernoulli}(\pi_{ij})\)`, for subject `\(i\)` and measurement `\(j\)`,
- random intercepts `\(b_i\)`, i.e., `\(b_i \sim N(0, d)\)`, can be included to capture the correlation.

The logit of `\(\pi_{ij}\)` is modeled as:

`$$\text{logit}(\pi_{ij}) =  \color{blue}{\beta_0} + \color{green}{\beta_1 Age_{ij}} + \color{red}{\beta_2 Gender_i} + \color{purple}{\beta_3 Gender_i Age_{ij}} + \color{magenta}{b_i}$$`

```r
fitGLMM &lt;- glmer(BMIBIN ~ sex + age + sex * age + (1 | ind),
                 data = Infant, family = binomial(link = "logit"), nAGQ = 25)
#print(round(summary(fitGLMM)$coefficients, 3))
```

---

#### Jimma infant: GEE vs GLMM Estimates

Regression coefficients are highly similar (a mathematical truth).
GLMM estimates a smaller intercept than GEE.

| Method | Parameter | Estimate (SE) | P-value |
|--------|-----------|---------------|---------|
| GEE    | Intercept | -1.869(0.112) | -       |
|        | Sex       | 1.133 (0.154) | 0.388   |
|        | age       | 0.001 (0.015) | 0.925   |
|        | Sex*age   | -0.017 (0.021)| 0.435   |
| GLMM   | Intercept | -2.333 (0.127)| -       |
|        | Sex       | 0.159 (0.164) | 0.331   |
|        | Sex*age   | 0.002 (0.015) | 0.895   |
|        | Sex*age   | -0.020 (0.021)| 0.333   |


- Should we prefer GLMM for this reason?

---

#### Model with random intercept and slope

The model with random intercept and slope can be fitted similarly. The following model is assumed for the mean structure:

- `\(Y_{ij} | b_i \sim \text{Bernoulli}(\pi_{ij})\)`, for subject `\(i\)` and measurement `\(j\)`
- Gaussian distributed random intercepts `\(b_i\)`, i.e., `\((b_{0i}, b_{1i}) \sim \text{N}(0, D)\)`, can be included to capture the correlation.

The logit of the probability `\(\pi_{ij}\)` is given by:

`$$\text{logit}(\pi_{ij}) = \color{blue}{\beta_0} + \color{green}{\beta_1 Age_{ij}} + \color{orange}{\beta_2 Gender_i} + \color{purple}{\beta_3 Gender_i Age_{ij}} + \color{red}{b_{0i}} + \color{teal}{b_{1i} Age_{ij}}$$`
---

#### Random intercept and slope model:


```r
fitGLMMSlope &lt;- glmer(BMIBIN~ sex + age + sex*age + (1+age|ind),
data = Infant, family = binomial(link = "logit"))
print(round(summary(fitGLMMSlope)$coefficients, 3))
```

```
##               Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)     -2.512      0.178 -14.077    0.000
## sexfemale       -0.173      0.199  -0.869    0.385
## age             -0.026      0.025  -1.038    0.299
## sexfemale:age    0.024      0.028   0.874    0.382
```

```r
# Extracting variance components for random effects
print(VarCorr(fitGLMMSlope))
```

```
##  Groups Name        Std.Dev. Corr  
##  ind    (Intercept) 1.90822        
##         age         0.24029  -0.708
```
---

#### Marginal model or GLMM – which should I prefer?

**Conceptual differences**

- **&lt;span style="color:blue"&gt;Population Risk&lt;/span&gt; vs &lt;span style="color:blue"&gt;Individual Risk:&lt;/span&gt;**
  - &lt;span style="color:blue"&gt;Marginal Model&lt;/span&gt; focuses on population-level risk.
  - &lt;span style="color:blue"&gt;GLMM&lt;/span&gt; accounts for individual-level risk.

- Choose a model that answers your scientific question:

- **&lt;span style="color:cyan"&gt;Modeling differences&lt;/span&gt;**

   - Covariance pattern or random effects model?
   - **&lt;span style="color:red"&gt;GEE&lt;/span&gt;** is more robust regarding model variations:
   - Choose a model that is realistic for your data:

- **&lt;span style="color:cyan"&gt;Computational differences&lt;/span&gt;**

  - Handling of missing data is easier in **&lt;span style="color:red"&gt;GLMMs:&lt;/span&gt;**
  - GLMMs become infeasible with large Data:
  - GEE standard errors are biased in small data:
  - Choose a model that can handle your data:

---
class: inverse,  middle

# Day 4

## Missing Data Management

* What is missing data 
   – definition, patterns, mechanisms (MCR, MR, NMR)

* Simple methods for handling missing data

* Multiple Imputation (MI) based procedures

* Weighted GEE

---

#### Introduction

- **&lt;span style="color:red"&gt;Missing data:&lt;/span&gt;** Absence of recorded information for certain observations or variables in a dataset.

- Missing data is very common in statistical analysis. 

- **&lt;span style="color:red"&gt;Challenges:&lt;/span&gt;** Biased results, reduced statistical power, loss of information in analysis.

- **&lt;span style="color:red"&gt;Impact:&lt;/span&gt;** Requires careful handling to avoid misleading conclusions.

- **&lt;span style="color:red"&gt;Importance:&lt;/span&gt;** Addressing missing data ensures accurate and reliable data analysis and interpretation.

---

#### Sources of Missing Data

- &lt;span style="color:red"&gt;Non-Response:&lt;/span&gt; Participants don't provide certain information.

- &lt;span style="color:red"&gt;Loss to Follow-Up:&lt;/span&gt; Participants drop out of a study before completion.

- &lt;span style="color:red"&gt;Measurement Error:&lt;/span&gt; Errors during data collection lead to missing values.

- &lt;span style="color:red"&gt;Skip Patterns:&lt;/span&gt; Questions skipped based on previous responses.

- &lt;span style="color:red"&gt;Data Entry Mistakes:&lt;/span&gt; Errors during data entry or coding.

- &lt;span style="color:red"&gt;Equipment Failure:&lt;/span&gt; Instruments or equipment malfunction during data collection.

- &lt;span style="color:red"&gt;Sensitive Data:&lt;/span&gt; Participants omit sensitive information.

- &lt;span style="color:red"&gt;Study Design:&lt;/span&gt; Data collection methods inherently result in gaps.

- &lt;span style="color:red"&gt;Natural Causes:&lt;/span&gt; Unforeseen events impacting data collection.

- &lt;span style="color:red"&gt;Processing Errors:&lt;/span&gt; Errors during data processing or transformation.

---

#### Implications of Missing Data

**Missing Data Produces/Induces:**

- Loss of information and &lt;span style="color:blue"&gt;reduced efficiency.&lt;/span&gt;

**Extent of Information Loss Depends on:**

- &lt;span style="color:red"&gt;Amount of missingness.&lt;/span&gt;
- &lt;span style="color:red"&gt;Missingness pattern.&lt;/span&gt;
- &lt;span style="color:red"&gt;Association between the missing and observed data.&lt;/span&gt;
- &lt;span style="color:red"&gt;Parameters of interest.&lt;/span&gt;
- &lt;span style="color:red"&gt;Method of analysis.&lt;/span&gt;

**Care is Needed to Avoid Biased Inferences:**

- Inferences that target a &lt;span style="color:purple"&gt;reference population other than intended.&lt;/span&gt;
- For example, &lt;span style="color:purple"&gt;those who stay in the study.&lt;/span&gt;
---

#### How Much Missing Data is "Problematic"?

Depends on who you ask...

- **Answer #1:**
  - ANY amount of missing data might be considered problematic.

- **Answer #2:**
  - It's never "too much."
  - Optimal methods can easily accommodate up to 50% missing data.

.pull-left[
- **Answer #3:**
  - `\(&gt;5\)`% (Schafer, 1999)
  - `\(&gt;10\)`% (Bennett, 2001)
  - `\(&gt;20\)`% (Peng, et al., 2006)
]
.pull-right[
- **Answer #4 (Widaman, 2006):**
  - 1%-2% (Negligible)
  - 5%-10% (Minor)
  - 10%-25% (Moderate)
  - 25%-50% (High)
  - `\(&gt;50\)`% (Excessive)
]

---

#### What to Consider

- **&lt;span style="color:red"&gt;Dealing with missing data requires considering&lt;/span&gt;** **&lt;span style="color:purple"&gt;the missing data patterns&lt;/span&gt;**, **&lt;span style="color:purple"&gt;mechanisms&lt;/span&gt;**, **&lt;span style="color:purple"&gt;proportion&lt;/span&gt;**, and the chosen analytic approach.

- &lt;span style="color:blue"&gt;Missing data pattern:&lt;/span&gt; examine the missing data pattern,(monotone, intermittent, or arbitrary structure), can guide the selection of suitable methods.

- &lt;span style="color:blue"&gt;Proportion of Missing data:&lt;/span&gt; High levels of missingness may impact the validity of analyses and may require more sophisticated handling techniques.

- &lt;span style="color:blue"&gt;Reasons for missing data:&lt;/span&gt; identifying the reasons can help mitigate potential biases.

- Ensure robust analyses in longitudinal studies when there is missing data.

---

#### Missing Data in Longitudinal Studies


**&lt;span style="color:purple"&gt;Monotone Missing Data Pattern&lt;/span&gt;**

- Missing data follows a consistent direction (either always increasing or always decreasing) across observations.

- Common in longitudinal studies where participants drop out progressively over time.

- May arise due to systematic reasons such as treatment effects or participant attrition.

- Example: Participants dropping out of a study as time progresses. 

| Study | Time1 | Time2 | Time3 |
|-------|-------|-------|-------|
| 1     | X     | X     | X     |
| 2     | X     | X     | .     |
| 3     | X     | .     | .     |

---

**&lt;span style="color:purple"&gt;Intermittent/Arbitrary Missing Data Pattern&lt;/span&gt;**

- Missing data occurs randomly or sporadically across observations without a consistent direction.

- Common in cross-sectional and longitudinal studies where participants may miss data points randomly.

- Can arise due to factors like random non-response or data collection errors.

- Example: Participants missing certain measurements for various reasons at different time points.


| Study | Time1 | Time2 | Time3 |
|-------|-------|-------|-------|
| 1     | X     | X     | X     |
| 2     | X     | .     | X     |
| 3     | .     | X     | X     |
| 4     | .     | .     | X     |
| 5     | .     | X     | .     |

---

#### Comparison:

**Data Collection Context:**
- Monotone: Often observed in longitudinal studies.
- Intermittent: Can occur in various study designs.

**Handling:**
- Monotone: Some methods like FOCF might be suitable.
- Intermittent: Requires more flexible imputation methods or techniques that account for random missingness.

**Bias:**
- Monotone: Biases can occur if the assumption of similarity between consecutive observations is violated.
- Intermittent: Imputed values might not be as biased as long as missingness is random.

---

#### Types of Missing Data

- Looking carefully the causes of missingness enable us to employee the **appropriate missing data management system**.

  - Estimation of the parameter with missing data depends on the missing data mechanism. 

- The missing data mechanism is a probability model for missingness. 
- Data is often described in accordance to **the reasons for the missing data**. 

- According to the mechanisms of missingness, we assume **three types** of missing data.
  - **&lt;span style="color:orange"&gt;Missing Completely at Random (MCAR)&lt;/span&gt;**
  
  - **&lt;span style="color:orange"&gt;Missing at Random (MAR)&lt;/span&gt;**
  
  - **&lt;span style="color:orange"&gt;Not Missing at Random (NMAR)&lt;/span&gt;**

---

#### 1. Missing Completely at Random (MCAR)

- In MCAR, missingness is assumed to be **&lt;span style="color:red"&gt;independent&lt;/span&gt; of both observed and unobserved data**.

- The notation of the MCAR mechanism is expressed as follows:

`$$\color{red}{p(R|Y) = p(R|Y^{obs}, Y^{mis}) = p(R)}$$`

- Where:

  - `\(Y\)` is a vector of partially observed data, that is, `\(Y =(Y^{obs}, Y^{mis})\)`.
  
  - `\(R\)` is a set of missing indicators, i.e., `\(R = 1\)` if the `\(j^{th}\)` element of `\(Y\)` is observed, and `\(R = 0\)` if the  `\(j^{th}\)` element of `\(Y\)` is missing (Rubin, 1976).

- MCAR is also known as **ignorable missing** in statistical inference.

---

**&lt;span style="color:red"&gt;Examples of MCAR data include:&lt;/span&gt;**

- Data with missing values due to equipment failure.

- Samples lost in transit.

- Data that is technically unsatisfactory.

**&lt;span style="color:purple"&gt; Characteristics of MCAR:&lt;/span&gt;**

- The missing data does not introduce any bias in statistical analyses.

- Estimated parameters are not biased as a result of the missing data.

**&lt;span style="color:red"&gt; Challenges of MCAR:&lt;/span&gt;**
  - Statistical power may be decreased due to the loss of information from the missing data.

---

#### 2. Missing at Random (MAR)

- Missingness depends only on observed components `\(Y^{obs}\)`, not on missing components `\(Y^{mis}\)`.
  - Expressed through the formula:
  
`$$\color{red}{p(R|Y^{obs}, Y^{mis}) = p(R|Y^{obs})}$$`
  
  - The missingness pattern is completely determined by the observed data.
  
  - The missingness mechanism does not depend on the actual missing values.

 - The missing data mechanism can be ignored in likelihood inference 
      - **Ignorable Missing Data Mechanism**

 - Estimates of parameters remain unbiased even with missing data.
 
---

- Examples
 
 - study protocol requires patients whose response value exceeds a threshold to be removed from the study
 - physicians give rescue medication to patients who do not respond to treatment
 
Features of &lt;span style="color: red;"&gt;MAR&lt;/span&gt; include:

- The observed data cannot be considered a random sample from the target population.
- Not all statistical procedures provide valid results under &lt;span style="color: red;"&gt;MAR&lt;/span&gt;.

| **&lt;span style="color: red;"&gt;Not Valid under MAR&lt;/span&gt;** | **&lt;span style="color: green;"&gt;Valid under MAR&lt;/span&gt;** |
|-------------------------------------------------------|-----------------------------------------------------|
| Sample Marginal Evolutions                             | Sample Subject-Specific Evolutions                   |
| Methods Based on Moments                               | Likelihood-Based Inference                           |
| Mixed Models with Misspecified Correlation Structure  | Mixed Models with Correctly Specified Correlation Structure |
| Marginal Residuals                                     | Subject-Specific Residuals                           |

---


#### 3. Not Missing at Random (NMAR)

- NMAR suggests that the probability of a value being missing fluctuates for &lt;span style="color:red"&gt;reasons unknown to us&lt;/span&gt;.

- NMAR occurs when the characteristics of missing data do not meet those of MCAR and MAR mechanisms.
- In NMAR, the probability of missingness is influenced by both the &lt;span style="color:red"&gt;observed value&lt;/span&gt; `\((Y^{obs})\)` and the &lt;span style="color:red"&gt;unobserved missing value&lt;/span&gt; `\((Y^{mis})\)`.

- The NMAR missing data mechanism can be represented by the following equation:
`$$p(R | Y^{obs}, Y^{mis}) = p(R | Y^{obs}, Y^{mis})$$`

- Due to the presence of this dependency, NMAR is considered a &lt;span style="color:red"&gt;non-ignorable missing data mechanism&lt;/span&gt;.

- Real-world examples are often subtle and &lt;span style="color: blue;"&gt;challenging to identify&lt;/span&gt;.

---

Examples

- in studies on drug addicts, people who return to drugs are less likely than others
to report their status
- in longitudinal studies for quality-of-life, patients may fail to complete the
questionnaire at occasions when their quality-of-life is compromised

Features of &lt;span style="color: red;"&gt;MNAR&lt;/span&gt; include:

- The observed data cannot be considered a random sample from the target population.

- Only procedures that explicitly model the joint distribution `\({y_i^o, y_i^m, R}\)` provide valid inferences.

- **&lt;span style="color: red;"&gt;Analyses that are valid under MAR will not be valid under MNAR.&lt;/span&gt;**

- We cannot distinguish from the data at hand whether the missing data mechanism is MAR or MNAR
   - We can distinguish between MCAR and MAR

- often use **&lt;span style="color:red"&gt;sensitivity analyses&lt;/span&gt;** and model-based imputation techniques.

---

**Common Methods of Missing Data Treatments for Longitudinal Data**

- **&lt;span style="color: red;"&gt;Simple Methods&lt;/span&gt;**
     - **Complete Case Analysis**: Analyzing only cases with &lt;span style="color: red;"&gt;complete data at all time points&lt;/span&gt;.
     - **Last Observation Carried Forward (LOCF)**: Imputing missing values with &lt;span style="color: red;"&gt;last observed value&lt;/span&gt;.

- **&lt;span style="color: red;"&gt;Modern Methods&lt;/span&gt;**
    - **Multiple Imputation**: Generating &lt;span style="color: green;"&gt;multiple plausible imputed datasets&lt;/span&gt;.
      - Captures uncertainty, provides &lt;span style="color: green;"&gt;accurate parameter estimates&lt;/span&gt;.
     
     - **Likelihood-based Methods**: Uses &lt;span style="color: blue;"&gt;full likelihood with missing data mechanism specified&lt;/span&gt;.
      - Requires &lt;span style="color: blue;"&gt;correct model specification&lt;/span&gt;, might be &lt;span style="color: blue;"&gt;complex for large datasets&lt;/span&gt;.
    
    - **Weighted GEE (Generalized Estimating Equations)**: &lt;span style="color: purple;"&gt;Accounts for correlation in repeated measurements&lt;/span&gt;.
      - Weighted analysis, effective for &lt;span style="color: purple;"&gt;longitudinal studies&lt;/span&gt;.
      
---

**&lt;span style="color: red;"&gt;Simple Methods&lt;/span&gt;**

**Complete Case Analysis**

   - &lt;span style="color: red;"&gt;Discards valuable information&lt;/span&gt;, can lead to &lt;span style="color: red;"&gt;biased results if missingness is related to unobserved variables&lt;/span&gt;.

- The most commonly used approach that data scientists use to deal with missing data is to simply omit cases with missing data, only analysing the rest of the dataset. 
- This method is known as listwise deletion or complete-case analysis.

- The **&lt;span style="color:magenta"&gt; na.omit()&lt;/span&gt;** function in R removes all cases with one or more missing data values in a dataset.

---

**Complete Case Analysis**

Example for *Complete Case Analysis (Listwise Deletion)* example using the `HIVdata` dataset.

First lets explore the missing value and pattern using 



```r
library(readr); library(naniar)
HIVdata &lt;- read_csv("Data/HIVdata.csv")
#miss_var_summary(HIVdata)
miss_var_table(HIVdata)
```

```
## # A tibble: 2 × 3
##   n_miss_in_var n_vars pct_vars
##           &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;
## 1             0      9       90
## 2           652      1       10
```

---
- We can also look the missing pattern at each time point 

```r
library(dplyr)
HIVdata %&gt;% select(cd4, time) %&gt;% group_by(time) %&gt;% miss_var_summary()
```

```
## # A tibble: 7 × 4
## # Groups:   time [7]
##    time variable n_miss pct_miss
##   &lt;dbl&gt; &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;
## 1     0 cd4           0      0  
## 2     6 cd4          52     15.5
## 3    12 cd4          82     24.5
## 4    18 cd4         118     35.2
## 5    24 cd4         127     37.9
## 6    30 cd4         131     39.1
## 7    36 cd4         142     42.4
```

---

- Now fit a linear mixed-effects model to the complete data:

.pull-left[
- **&lt;span style="color: red;"&gt;Before remove na's&lt;/span&gt;**

```r
fit1 &lt;- lmer(cd4 ~ time + (1 | id), 
             data = HIVdata)
summary(fit1)
```

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: cd4 ~ time + (1 | id)
##    Data: HIVdata
## 
## REML criterion at convergence: 21204
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.7303 -0.5130 -0.0152  0.4594  5.8864 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  id       (Intercept) 20542    143.3   
##  Residual             10043    100.2   
## Number of obs: 1693, groups:  id, 335
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 208.0725     8.8094   23.62
## time          6.5915     0.2058   32.04
## 
## Correlation of Fixed Effects:
##      (Intr)
## time -0.360
```
]

.pull-right[
- **&lt;span style="color: red;"&gt;After removal na's&lt;/span&gt;**

```r
cc_data &lt;- na.omit(HIVdata)
cc_model&lt;- lmer(cd4 ~time +(1|id), 
              data = cc_data)
summary(cc_model)
```

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: cd4 ~ time + (1 | id)
##    Data: cc_data
## 
## REML criterion at convergence: 21204
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.7303 -0.5130 -0.0152  0.4594  5.8864 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  id       (Intercept) 20542    143.3   
##  Residual             10043    100.2   
## Number of obs: 1693, groups:  id, 335
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 208.0725     8.8094   23.62
## time          6.5915     0.2058   32.04
## 
## Correlation of Fixed Effects:
##      (Intr)
## time -0.360
```
]

---

- **Last Observation Carried Forward (LOCF) Method**

  - **Replace Missing Values**: In LOCF, missing values are &lt;span style="color: blue;"&gt;imputed by carrying forward the last observed value&lt;/span&gt;. 
        - Assumes unrealistic constant profile after dropout, introduces &lt;span style="color: red;"&gt;bias&lt;/span&gt;.
  - **Bias Concerns**: LOCF **&lt;span style="color: red;"&gt;introduces bias&lt;/span&gt;** by assuming the last observed value accurately represents the participant's status. However, this disregards potential changes or fluctuations after the last observation, impacting analysis validity and conclusions.

- To perform the LOCF imputation method, we simply run the 
  - **&lt;span style="color:orange"&gt;fill()&lt;/span&gt;** function from the tidyr package on the dataset and the variable with the missing data.
  - **&lt;span style="color:magenta"&gt;lna.locf()&lt;/span&gt;** function in zoo

---


```r
library(zoo)
LOCF_data2 &lt;- na.locf(HIVdata)
model_locf2 &lt;- lmer(cd4 ~ time + (1 | id), data = LOCF_data2)
summary(model_locf2)
```

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: cd4 ~ time + (1 | id)
##    Data: LOCF_data2
## 
## REML criterion at convergence: 30694.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.6241 -0.5259 -0.0920  0.4296  7.2089 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  id       (Intercept) 12410    111.4   
##  Residual             22684    150.6   
## Number of obs: 2345, groups:  id, 335
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 223.1570     8.2756   26.97
## time          6.2135     0.2592   23.97
## 
## Correlation of Fixed Effects:
##      (Intr)
## time -0.564
```

---

#### Mean Imputation in Longitudinal Data

- **Mean Imputation**: A &lt;span style="color: blue;"&gt;simple method&lt;/span&gt; to handle missing values in longitudinal data.

- **Approach**: Replace missing values with the &lt;span style="color: green;"&gt;mean of the observed values&lt;/span&gt; for the same variable across time points.

- **&lt;span style="color: green;"&gt;Simple and straightforward method&lt;/span&gt;** to handle missing values.
- Easy to implement and **&lt;span style="color: green;"&gt;interpret&lt;/span&gt;**.
- **&lt;span style="color: green;"&gt;Maintains the structure&lt;/span&gt;** of the dataset.

- May **&lt;span style="color: red;"&gt;introduce bias&lt;/span&gt;** if the missing data mechanism is not missing completely at random (MCAR).
- Does not account for individual variation or trends over time.
- Reduces **&lt;span style="color: red;"&gt;variability in the imputed variable&lt;/span&gt;**, potentially underestimating uncertainty.

---

#### Example


```r
library(dplyr)
# Replace missing values with cd4 means
imputed_data &lt;- HIVdata %&gt;%  group_by(id) %&gt;%
  mutate(cd4 = ifelse(is.na(cd4), mean(cd4, na.rm = TRUE), cd4))
model_mean_imputed &lt;- lmer(cd4 ~ time + (1 | id), data = imputed_data)
summary(model_mean_imputed)
```

```
## Linear mixed model fit by REML ['lmerMod']
## Formula: cd4 ~ time + (1 | id)
##    Data: imputed_data
## 
## REML criterion at convergence: 28803.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -6.4796 -0.5200 -0.0299  0.4747  6.4495 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  id       (Intercept) 21159    145.46  
##  Residual              8342     91.34  
## Number of obs: 2345, groups:  id, 335
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 225.5415     8.6443   26.09
## time          4.6201     0.1572   29.39
## 
## Correlation of Fixed Effects:
##      (Intr)
## time -0.327
```


---

### Multiple Imputation

- Multiple imputation is a process that is done in 3 main steps: **&lt;span style="color:blue"Imputation, analysis, and pooling.&lt;/span&gt;**

- This gives the imputed data a valid statistical inference.

&lt;span style="color:red"&gt;Steps for Multiple Imputation:&lt;/span&gt;

- Firstly, generate m multiple imputed datasets.

- Secondly, analyze each imputed dataset, then there should be m analyses.

- Lastly, combine the results for the pooled dataset.

- Multiple imputation is robust to small sample sizes or lots of missing data.

---

- Steps in applying multiple imputation to missing data via the `mice` approach

&lt;img src="Image/MI.png" width="60%" style="display: block; margin: auto;" /&gt;

- The estimate of the parameter `\(\beta\)` is simply the average of each parameter estimate `\(\beta^m\)` obtained over the m imputed datasets (m = 1, ..., M):$$\hat{\beta}^* = \frac{1}{M}\sum_{m=1}^M \hat{\beta}^m$$

---

- The variance of the estimator is partitioned into within imputation variance (sampling variability), and the between imputation variance (estimation variability due to missing data).

- The within imputation variance, `\(W_{\beta}\)`, over the m imputed datasets is:
`$$W_{\beta}= \frac{\sum_{m=1}^M SE_{\beta}^2}{M}$$`
- The between imputation variance, `\(B_{\beta}\)`, over the m imputed datasets is: `$$B_{\beta}= \frac{\left(\sum_{m=1}^M\left(\hat{\beta}^m-\hat{\beta}^*\right)^2\right)}{M-1}$$`

- These two variances are combined to provide a single variance, given by
`$$T_{\beta}=W_{\beta}+ \left[\frac{(M+1)}{M}\right] B_{\beta}$$`

---

- Let's look at it using the mice() function 


```r
library(mice)
data.mice &lt;- mice(HIVdata, m=5, method = "pmm", printFlag=FALSE, print = FALSE)
```

- m=5 is to generate 5 imputed data,

- different prediction model was used for different type of variable
    
   - pmm: predicted mean matching
   - logreg: logistic regression
   - polr: ordinal logistic regression

- usually, one needs only 5 replicates for model building/testing, 20 to 100 for final model

---


```r
library(broom.mixed)
mice.fit &lt;- with(data.mice, lmer(cd4 ~ time + (1 | id)))
summary(pool(mice.fit))
```

```
##          term   estimate std.error statistic       df      p.value
## 1 (Intercept) 220.832111 9.2215521  23.94739 54.13070 1.769731e-30
## 2        time   6.010341 0.3306921  18.17504 24.41684 1.068857e-15
```

---

- Now we can extract the completed dataset using the complete() function. 


```r
library(tidyr)
impdata &lt;- mice::complete(data.mice, action = "long", inc = F)
#View(impdata)
```

- The missing values have been replaced with the imputed values in the first of the five datasets. 

---

#### Multiple Imputation Software 

- **`Amelia`** in R (by Gary King and collaborators)
- **`mi`** in R (by Andrew Gelman and collaborators)
- **`mice`** in R (by Stef van Buuren and collaborators) 
- SPSS **`(Analyze &gt; Multiple Imputation)`**
- STATA  **mi `estimate`**



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"ratio": "14:9",
"highlightSpans": true,
"highlightStyle": "github",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
